[
["index.html", "Classical approaches to Machine Learning 1 About the course 1.1 Prerequisites 1.2 Schedule 1.3 Github 1.4 License 1.5 Contact 1.6 Colophon", " Classical approaches to Machine Learning Chris Penfold 2022-10-28 1 About the course The term “Machine Learning” encompasses a broad range of approaches in data analysis with wide applicability across biological sciences. Lectures will introduce commonly used approaches, provide insight into their theoretical underpinnings and illustrate their applicability and limitations through examples and exercises. During the practical sessions students will apply the algorithms to real biological data-sets using the R language and RStudio environment. All code utilised during the course will be available to participants. 1.1 Prerequisites Some familiarity with R would be helpful. 1.2 Schedule Time Data Module 14:00 – 15:00 07/11/23 Introductory lecture (linear regression) 14:00 – 15:00 08/11/23 Linear regression / linear models 14:00 – 15:00 11/11/23 Logistic regression 14:00 – 15:00 14/11/23 Intro lecture (neural networks) 14:00 – 15:00 15/11/23 Convolutional neural nets and beyond 14:00 – 15:00 18/11/23 D3: Interpreting NNs (ChIP motif case study) 14:30 – 15:30 21/11/23 Practical Q&amp;A session 1.3 Github The github reposotory for Classical approaches to Machine Learning containing code, datasets and lectures is availabile here. The html textbook is found in docs/index.html. Individual chapters (files ending .Rmd) can be opened in RStudio as interactive markdown files. The course material can be viewed online here. 1.4 License GPL-3 1.5 Contact If you have any comments, questions or suggestions about the material, please contact Chris Penfold. 1.6 Colophon This book was produced using the bookdown package (Xie 2017), which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "2 Introduction", " 2 Introduction In this workbook, we cover two themes. We begin in chapter 5 with linear regression and logistic regression. Here we will explore how to use these statistical methods in the context of machine learning. The majority of the excercises are based on a plant infection dataset. We will see how regression models of different complexity can be fit to the dataset, and how held out data can be used to choose the best model. After this, we will see how logistic regression approaches can be used to pick out marker genes that indicate infected status. Finally, we explore how regression approaches can be used in the context of more recent datasets: inference of spatially-resolved gene expression patterns in a developing embryo. In chapter 6 we introduce the concepts of multilayer neural networks. We demonstrate how neural networks can be built for regression applications. Later we introduce Convolutional Neural Networks (CNN) and see how they can be used for image recognition. In this chapter we build an algorithm capable of recognising a well known cartoon characer in a set of images. Finally, we briefly discuss how these basic approaches can be built into more complex algorithms. "],
["installation.html", "3 Installation", " 3 Installation For the second half of this workbook we will make use of an R wrapper for keras (a Python package, which itself backends to Tensorflow). To be able to do so, we will have to install Python (I have tested on Python 3.9), tensorflow and keras. Python can be installed from here. When installing Python there will be an option to add the package to the system path: choosing to do so will make things a lot easier when it comes to running the backend in R. Depending on your operating system, it may also be necessary to allow long long path names (this seems to be a Windows option). If you do not allow long path names, tensorflow may not install. If you did not enable Long paths, this will have to be set manually, and instructions can be found on Google by searching: How do I enable enabled Win32 long paths? Once Python is installed, it will be necessary to idenity how to call it from the Terminal/Command Line. Usually, if you have addedd the path on installation this will be the Python version i.e., I can launch the Python3.9 from Terminal/Command Line via: python3.9 This will usually work if you have multiple versions installed, but sometimes names can be mixed up. For example, I installed Python3.9 and Python 3.6 on a Windows machine, and could call the former with python3 but had to call: python to open Python3.6. The version of Python you have launched will usually be displayed on launch. Once you've identified how to launch the specific version of Python you want, the next step is to install tensorflow to that version. Sometimes when you have multiple installs it can be difficult to ensure the correct pip is called. To avoid this confusion, you can be explicit: python3.9 -m pip install tensorflow You can be even more specific by selecting a version of tensorflow: python3.9 -m pip install tensorflow==2.7.0 Tensorflow 2.2.0 is the version I have installed on my machine. Keras has already been incorporated into the most recent versions of tensorflow, and so it may not be necesary to install a seperate version of keras. For debugging purposes I did not install keras. You can check things have installed within Python by launching a python instance and loading the packages: python3.9 Then from within Python import tensorflow as tf import keras tf.version.VERSION keras.__version__ exit() Finally, we will open Rstudio, install reticulate, set the version of Python to use, and install the R backend to tensorflow (and keras depending on the verion you have installed). A more complete installation for the R keras/tensorflow packages can be found here. install.packages(&quot;reticulate&quot;) library(reticulate) use_python(&quot;Python3.9&quot;) install.packages(&quot;tensorflow&quot;) #install.packages(&quot;keras&quot;) In some cases, particularly for Windows, you might have to resort to giving reticulate the full path name to the Python executable. You can find our where your Python executables are by typing in the terminal: where python You should then get a full path to the various insalls and can use this in R, which will look something like: install.packages(&quot;reticulate&quot;) library(reticulate) use_python(&quot;C:/Program Files/Python39/python.exe&quot;) install.packages(&quot;tensorflow&quot;) #install.packages(&quot;keras&quot;) At this stage you should now be ready to run Keras in R. "],
["course-materials.html", "4 Course materials", " 4 Course materials The course materials can be viewed online here. A complete copy of the course (with all datasets, lecture slides, and compiled materials) can be accessed via GitHub [here](here. By default, Git is installed on linux and macOS. To clone a copy on macOS use terminal navigate to the directory you want to use and type: git pull https://github.com/cap76/AZCourse_Winter2022.git For Windows machines you may have to set up Git. Alternatively, I have uploaded a copy of the complete folder on GoogleDrive. More information about Python installations can be found at the links below. Installing tensorflow/keras for R Installing Python Linux Installing Python for Mac Installing Python via Conda Installing Tensorflow Installing Keras "],
["logistic-regression.html", "5 Linear regression and logistic regression 5.1 Regression 5.2 Using regression approaches to infer 3D gene expression patterns in marmoset embryos 5.3 Resources", " 5 Linear regression and logistic regression In section 5.1 we briefly recap linear regression. As a real-world example we demonstrate the use of linear regression to predict gene expression values as a function of time. In this section we also demonstrate how, by breaking data into training and test sets, we can choose between models of increasing complexity in order to select one that is optimal in terms of predictive accuracy. Such models can be used to make predictions about the future (or at intermediate points lacking data) which may form the basis for automated decision making. In section 5.1.3 we recap logistic regression (section 5), and demonstrate how such approaches can be used to predict pathogen infection status in Arabidopsis thaliana. By doing so we identify key marker genes indicative of pathogen growth. 5.1 Regression To recap our understanding of regression we will make use of an existing dataset which captures the gene expression levels in the model plant Arabidopsis thaliana following inoculation with Botrytis cinerea (Windram et al. 2012), a necrotrophic pathogen considered to be one of the most important fungal plant pathogens due to its ability to cause disease in a range of plants. Specifically this dataset is a time series measuring the gene expression in Arabidopsis leaves following inoculation with Botrytis cinerea over a \\(48\\) hour time window, with observations taken at \\(2\\) hour intervals (see 5.1). Whilst this example is biological in motivation the methods we discuss should be general and applicable to other collections of time series data, and it may be helpful to instead think of things in terms of input variables and output variables. Figure 5.1: Botrytis infection of Arabidopsis over a 48 hour window The dataset is available from GEO (GSE39597) but a pre-processed version has been deposited in the data folder. This pre-processed data contains the expression levels of a set of \\(163\\) marker genes in tab delimited format. The fist row contains gene IDs for the marker genes (the individual input variables). Column \\(2\\) contains the time points of observations, with column \\(3\\) containing a binary indication of infection status evalutated as \\(0\\) or \\(1\\) according to wether there was a detectable presence of Botrytis cinerea tubulin protein. All subsequent columns indicate (\\(\\log_2\\)) normalised Arabidopsis gene expression values from microarrays (V4 TAIR V9 spotted cDNA array). The expression dataset itself contains two time series: the first set of observations represent measurements of Arabidopsis gene expression in a control time series (uninfected), from \\(2h\\) through \\(48h\\) at \\(2\\)-hourly intervals, and therefore capture dynamic aspects natural plant processes, including circadian rhythms; the second set of observations represents an infected dataset, again commencing \\(2h\\) after inoculation with Botyris cinerea through to \\(48h\\). Both conditions are replicated a number of times. Within this section our question is usually framed in the form of &quot;how does this gene's expression change over time.&quot; The output variable will typically be the expression level of a gene of interest, denoted \\(\\mathbf{y} =(y_1,\\ldots,y_n)^\\top\\), with the explanatory variable being time, \\(\\mathbf{X} =(t_1,\\ldots,t_n)^\\top\\). We can read the dataset into {R} as follows: D &lt;- read.csv(file = &quot;data/Arabidopsis/Arabidopsis_Botrytis_pred_transpose_3.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) To take a look at the data in the R environment simply type the name of the variable: From this we can see for ourself that the data consists of several variables measured over a time course. In fact, this experiment consists of several time series, with measurements of Arabidopsis leaves in response to infection with a necrotophic fungus , and a second set of experiments containing gene expression in an uninfected (control) conditions. Each condition has 4 replicates, so \\(8\\) time-series in total. The variables are represented columnwise, including time and gene experssion, all of which are continuous variables. Two variables, labeled as Class' andInfec' appear to be binary - we will make use of these later. We can extract out the names of the variables (mostly gene names) as a new variable in R, by taking the column names: genenames &lt;- colnames(D) We can also pull out the time variables of the control time series. From the structure of the data we know that the first \\(96\\) rows correspond to control (\\(4\\) sets of \\(24\\)), with the second \\(96\\) corresponding to infection. Xs &lt;- D$Time[1:96] whilst for the treatment the times would be: Xs2 &lt;- D$Time[97:nrow(D)] Another way we can pull out data is to rely on indexing. For example if we did: timeind &lt;- which(genenames==&quot;Time&quot;) This would tell us which colum contains the variable `Time'. We could then pull out the data: genenames &lt;- colnames(D) Xs2 &lt;- D[97:nrow(D),timeind] which is exactly the same as line 41. Before we get down to doing any real Machine Learning we first need to familiarise ourself with the data. In fact, it helps a lot if we come armed with a well thought out question: this will help us generate optimal datasets to begin with (or at the very least steer which datasets we will use), and will guide what methods we use to analyse the dataset. As previously suggested, our question going forward will be something like `how does gene expression change over time and in response to infection'. Let's start by plotting one of the gene expression variables (AT2G28890) as a function of time. The standard plotting we used throughout this course will be ggplot. It makes for very nice plotting, but can be sometimes be a little obscure in syntax, so the code below is probably more opaque than is necessary. library(ggplot2) ggplot(D, aes(x = Time, y = AT2G28890, colour = factor(Class)) ) + geom_point(size=2.5) + theme_bw() So here Time' is our explanatory variable, the variable that is generally easy to measure, andAT2G28890' represents our output variable, the one we're actually interested in. In the above plot we can see both a change in the variable over time, and a striking difference between the control versus infected time series. Depending on the number of variables we could do this for each variable in turn, but this would be tedious for larger datasets when we have thousands or even tens of thousands of variables. A heatmap is a good way to visualise many variables simultaneously. In fact, let's take a look at the heatmap of the infected time series minus the control using the `pheatmap' function. For ease of interpretation we will do this for replicate one only: library(pheatmap) DeltaVals &lt;- t(D[97:120,3:164] - D[1:24,3:164]) #Here we subtract the expression of the control from infected for replicate 1 pheatmap(DeltaVals, cluster_cols = FALSE, cluster_rows = TRUE) In the above snippet we have additionally clustered the values to bring out the signal even more. We can clerly see strong patterns in the data that show both up-regulation and down-regulation of genes over time. This is the beginning of an exploratory analysis we might do to gauge wether the dataset contains useful information - only then might we begin to use ML to ask questions of it. In the next section we will undertake a very simple task: we will focus on the gene AT2G28890 and in either the control or infection time series we will try to identify the functional nature of the expression pattern. 5.1.1 Linear regression Now that we have an idea about what our dataset is, and are sure of its quality, we can start to do something with it. Here we have a time series (a number of time-series, in fact), and want to develop an understanding of how specific genes are changing over time: this would allow us to predict what gene expression might be doing at some point in the future (forecasting) or uncover something about the physical nature of the system i.e., what kind of function best describes the behavior. To do so we first need a model for how we expect the variable to behave. One of the simplest models we could assume is linear regression, which assumes that the variable of interest, denoted \\(y\\), depends on an explanatory variable, \\(x\\), via: \\(y = m x + c.\\) For a typical set of data, we have a vector of observations, \\(\\mathbf{y} = (y_1,y_2,\\ldots,y_n)\\) with a corresponding set of explanatory variables. For now we can assume that the explanatory variable is scalar, for example time (in hours), such that we have a set of observations, \\(\\mathbf{X} = (t_1,t_2,\\ldots,t_n)\\). Using linear regression we aim to infer the parameters \\(m\\) and \\(c\\), which will tell us something about the relationship between the two variables, and allow us to make predictions at a new set of locations, \\(\\mathbf{X}*\\). But how do we infer these parameters? The answer is we do so by empirically minimising/maximising some objective function, for example the sum squared error. Specifically, for a given value of \\(m\\) and \\(c\\) we can make predictions about what the value of \\(y\\) is for any given vallue of \\(x\\), which we can then compare to a measured value. We therefore split that data into two: a training set, \\(\\{ \\mathbf{X}_{train}, \\mathbf{y}_{train}\\}\\), and a test set, \\(\\{ \\mathbf{X}_{test}, \\mathbf{y}_{test}\\}\\). Using the training set we can can we can find a value of \\(m\\) and \\(c\\) such that the sum of the squared difference between predictions of the model at locations \\(\\mathbf{X}_{train}\\), denoted \\(\\mathbf{y}^\\prime\\), and the actual observed values \\(\\mathbf{y}_{train}\\) are in some way minimal. A number of other objective functions exist, each of which comes with their own set nuances. A key benefit of using the sum squared error in this case is that optimisation is mathematically tractable: that is we can directly solve the equation rather than having to do iterative searches. Within R, all linear regression can be implemented via the lm function. In the example below, we perform linear regression for the gene expression of AT2G28890 as a function of time, using \\(3\\) of the \\(4\\) infection time series (saving the fourth for validation): linmod &lt;- lm(AT2G28890~Time, data = D[4*24 +1:8*24,]) Here the {lm} function has analytically identified the gradient and offset (\\(m\\) and \\(c\\) parameters) based upon all 24 time points (4 replicates), and we can take a look at those parameters via {linmod$oefficients}. In general, it is not a very good idea to infer parameters using all of the data. Doing so would leave no way to choose betwee different models and evaluate for overfitting. Ideally, we wish to partition the dataset into a training set, and an evaluation set, with parameters evaluated on the training set, and model performance summarised over the evaluation set. We can of course partition this dataset manually, or use a package to do so. The {caret} package is a machine learning wrapper that allows easy partitions of the dataset. Linear regression is implemented within the {caret} package, allowing us to make use of these utilities. In fact, within caret, linear regression is performed by calling the function lm. In the example, below, we perform linear regression for gene AT2G28890, and predict the expression pattern for that gene using the {predict} function: library(caret) ## Warning: package &#39;caret&#39; was built under R version 3.5.2 ## Loading required package: lattice ## Warning: package &#39;lattice&#39; was built under R version 3.5.2 library(mlbench) library(ggplot2) set.seed(1) geneindex &lt;- which(genenames==&quot;AT2G28890&quot;) startind &lt;- (4*24)+1 endind &lt;- 7*24 xtrain = D[startind:endind,1] ytrain = D[startind:endind,geneindex] lrfit &lt;- train(y~., data=data.frame(x=xtrain,y=ytrain ), method = &quot;lm&quot;) predictedValues&lt;-predict(lrfit) Note that here we have again manually selected the first three replicates from the infection time series (indexed by rows \\(97-168\\)) and thus have saved replicate \\(4\\) for evaluating performance. As an alternative, we could have instead randomly partitioned the data into a training set and test set, although there is no exact prescirption for doing so, and anthing between a \\(60/40\\) and \\(80/20\\) split is common. If we went donwn this route, our code would look something like: and voila, we have our training and test sets. Alternative way we could split the data is via the createDataPartition function: An important side note is that here is that, on lines 101 we have set the random number generator to help ensure our code is repeatable. Another thing we will need to do to help make things more repeatable is to take note of what package numbers we used. We can do so by printing the session info: print(sessionInfo()) ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] mlbench_2.1-1 caret_6.0-86 lattice_0.20-40 ggplot2_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.4 lubridate_1.7.4 listenv_0.8.0 ## [4] class_7.3-15 digest_0.6.25 ipred_0.9-12 ## [7] foreach_1.5.1 utf8_1.1.4 parallelly_1.23.0 ## [10] R6_2.5.0 plyr_1.8.6 stats4_3.5.1 ## [13] evaluate_0.14 highr_0.8 pillar_1.6.0 ## [16] rlang_0.4.10 lazyeval_0.2.2 data.table_1.12.8 ## [19] jquerylib_0.1.3 rpart_4.1-15 Matrix_1.2-18 ## [22] rmarkdown_2.7 labeling_0.4.2 splines_3.5.1 ## [25] gower_0.2.1 stringr_1.4.0 munsell_0.5.0 ## [28] compiler_3.5.1 xfun_0.12 pkgconfig_2.0.3 ## [31] globals_0.14.0 htmltools_0.5.1.1 nnet_7.3-13 ## [34] tidyselect_1.1.1 tibble_3.1.1 prodlim_2019.11.13 ## [37] bookdown_0.20 codetools_0.2-18 fansi_0.4.1 ## [40] future_1.21.0 crayon_1.4.1 dplyr_1.0.5 ## [43] withr_2.4.1 ModelMetrics_1.2.2.2 MASS_7.3-51.5 ## [46] recipes_0.1.17 grid_3.5.1 nlme_3.1-145 ## [49] jsonlite_1.6.1 gtable_0.3.0 lifecycle_1.0.0 ## [52] DBI_1.1.1 magrittr_1.5 pROC_1.16.2 ## [55] scales_1.1.1 future.apply_1.7.0 stringi_1.4.6 ## [58] reshape2_1.4.3 farver_2.0.3 timeDate_3043.102 ## [61] bslib_0.2.5.1 ellipsis_0.3.0 generics_0.1.0 ## [64] vctrs_0.3.8 lava_1.6.10 iterators_1.0.13 ## [67] tools_3.5.1 glue_1.3.2 purrr_0.3.3 ## [70] parallel_3.5.1 survival_3.1-11 colorspace_1.4-1 ## [73] knitr_1.28 sass_0.4.0 Or look at a specific package: packageVersion(&quot;ggplot2&quot;) ## [1] &#39;3.2.1&#39; A summary of the model, including parameters, can be printed out to screen using the {summary} function: summary(lrfit) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3862 -0.3787 0.0814 0.4267 1.7164 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.380430 0.201695 51.466 &lt; 2e-16 *** ## x -0.062616 0.007058 -8.872 4.54e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8291 on 70 degrees of freedom ## Multiple R-squared: 0.5293, Adjusted R-squared: 0.5226 ## F-statistic: 78.71 on 1 and 70 DF, p-value: 4.543e-13 Returning to our task, we might ask how well the model has fitted the data. Conveniently, in cases where we do not specify otherwise, {caret} will perform \\(k\\)-fold cross validation on the training set, and we can look at various metrics on the held out data in {lrfit$results}. We can also make predictions at new points (for example if we are interested in forecasting at some time in the future) by specifying a new set of time points over which to make a prediction: newX &lt;- seq(0,48,by=0.5) forecastValues&lt;-predict(lrfit,newdata = data.frame(x=newX) ) ggplot(data.frame(x=xtrain,y=ytrain ), aes(x = x, y = y)) + geom_point(size=2.5) + geom_point(color=&#39;blue&#39;) + geom_line(color=&#39;blue&#39;,data = data.frame(x=newX,y=forecastValues), aes(x=x, y=y)) + theme_bw() In general the fit seems to capture a general downward trend. We can also take a look at predictions in the held-out \\(4\\)th replicate: newX &lt;- D[169:192,1] forecastValues&lt;-predict(lrfit,newdata = data.frame(x=newX) ) residuals &lt;- forecastValues - D[169:192,geneindex] plot(residuals, type=&quot;p&quot;,col=&quot;black&quot;,main=genenames[geneindex]) ggplot(data.frame(x=newX,y=residuals ), aes(x = x, y = y)) + geom_point(size=2.5) + geom_point(color=&#39;blue&#39;) + theme_bw() By and large, for a good model, we would expect the residuals to look roughly random centred on \\(0\\). If we see structure, this may be a clue that our model is not as useful as it could be. We can also summarise performence by e.g., calculating the root mean squared error on the held out data: RMSE &lt;- sqrt( mean( (forecastValues - D[169:192,geneindex])^2 ) ) The error on held out data comes into its own when looking to compare models, as we shall see in the next section. Finally, let's also fit a linear model to the control dataset (again only using 3 replicates), and plot the inferred results alongside the observation data for both fitted models: newX &lt;- seq(0,48,by=0.5) lrfit2 &lt;- train(y~., data=data.frame(x=D[1:72,1],y=D[1:72,geneindex]), method = &quot;lm&quot;) lrfit &lt;- train(y~., data=data.frame(x=D[97:168,1],y=D[97:168,geneindex]), method = &quot;lm&quot;) predictedValues2 &lt;- predict(lrfit2, newdata = data.frame(x=newX)) predictedValues&lt;-predict(lrfit,newdata = data.frame(x=newX) ) ggplot(D, aes(x = Time, y = AT2G28890, colour = factor(Class))) + geom_point(size=2.5) + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) + geom_line(color=&#39;red&#39;,data = data.frame(x=newX,y=predictedValues2), aes(x=x, y=y)) + geom_line(color=&#39;blue&#39;,data = data.frame(x=newX,y=predictedValues), aes(x=x, y=y)) + theme_bw() Whilst the above model appeared to do reasonably well at capturing the general trends in the dataset, if we take a closer look at the control data (in red), you may notice that, visually, there appears to be more structure to the data than indicated by the model fit. One thing we can do is take a look at the residuals fo each model: if there is structure in the residuals, it would suggest the model is not capturing the full richness of the model. Indeed, if we look AT2G28890 up on CircadianNET, we will see it is likely circadian in nature (\\(p&lt;5\\times10^{-5}\\)) suggesting there may be some rhythmicity to it. To better accommodate the complex nature of this data we may need something more complicated. 5.1.2 Polynomial regression In general, linear models will not be appropriate for a large variety of datasets, particularly when the variables of interest are nonlinear. We can instead try to fit more complex models, such as a quadratic function, which has the following form: \\(y = m_1 x + m_2 x^2 + c,\\) where \\(m = [m_1,m_2,c]\\) represent the parameters we're interested in inferring. An \\(n\\)th-order polynomial has the form: \\(y = \\sum_{i=1}^{n} m_i x^i + c.\\) where \\(m = [m_1,\\ldots,m_n,c]\\) are the free parameters. As before, the goal is to try to find values for these parameters such that we maximise/minimise some objective function. Within R we can infer more complex polynomials from the data using the {lm} package by calling the {poly} function when specifying the symbolic model. In the example below we fit a \\(3\\)rd order polynomial (the order of the polynomial is specified via the {degree} variable): lrfit3 &lt;- lm(y~poly(x,degree=3), data=data.frame(x=D[1:72,1],y=D[1:72,geneindex])) We can agin do this in caret: in the snippet, below, we fit \\(3\\)rd order polynomials to the control and infected datasets, and plot the fits alongside the data. lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[1:72,1],y=D[1:72,geneindex]), method = &quot;lm&quot;) lrfit4 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[97:168,1],y=D[97:168,geneindex]), method = &quot;lm&quot;) newX &lt;- seq(0,48,by=0.5) predictedValues&lt;-predict(lrfit3,newdata = data.frame(x=newX) ) predictedValues2 &lt;- predict(lrfit4, newdata = data.frame(x=newX)) ggplot(D, aes(x = Time, y = AT2G28890, colour = factor(Class))) + geom_point(size=2.5) + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) + geom_line(color=&#39;blue&#39;,data = data.frame(x=newX,y=predictedValues2), aes(x=x, y=y)) + geom_line(color=&#39;red&#39;,data = data.frame(x=newX,y=predictedValues), aes(x=x, y=y)) + theme_bw() Note that, by eye, the fit appears to be a little better than for the linear regression model. Well, maybe! We can quantify the accuracy of the models by looking at the root-mean-square error (RMSE) on the hold-out data (test-set), defined as: \\(\\mbox{RMSE} = \\sqrt{\\sum_{i=1}^n (\\hat{y_i}-y_i)^2/n}\\) where \\(\\hat{y_i}\\) is the predicted value (model prediction) and \\(y_i\\) the observed value of the \\(i\\)th (held out) datapoint. What happens if we fit a much higher order polynomial? Try fitting a polynomial with degree up to \\(d = 10\\) and plotting the result. lrfit3 &lt;- train(y~poly(x,degree=12), data=data.frame(x=D[1:72,1],y=D[1:72,geneindex]), method = &quot;lm&quot;) lrfit4 &lt;- train(y~poly(x,degree=12), data=data.frame(x=D[97:168,1],y=D[97:168,geneindex]), method = &quot;lm&quot;) newX &lt;- seq(0,48,by=0.5) predictedValues&lt;-predict(lrfit3,newdata = data.frame(x=newX) ) predictedValues2 &lt;- predict(lrfit4, newdata = data.frame(x=newX)) ggplot(D, aes(x = Time, y = AT2G28890, colour = factor(Class))) + geom_point(size=2.5) + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) + geom_line(color=&#39;blue&#39;,data = data.frame(x=newX,y=predictedValues2), aes(x=x, y=y)) + geom_line(color=&#39;red&#39;,data = data.frame(x=newX,y=predictedValues), aes(x=x, y=y)) + theme_bw() As we increase the model complexity the fit may appear to match perfectly well to the training set. However, such models become completely useless for prediction purposes. We are overfitting! This is why we use held out data, so that we can evaluate, empirically, when a model is useful, or when it is simply memorising the training set (noise and nuance and all). Using our gene of interest explore the model complexity i.e., try fitting polynomial models of increasing complexity. Plot the RMSE on the test set as a function of degree. Which model fits best? In the code below we systematically fit a model with increasing degree and evaluate/plot the RMSE on the held out data. xtrain &lt;- D[1:72,1] ytrain &lt;- D[1:72,geneindex] xtest &lt;- D[73:96,1] ytest &lt;- D[73:96,geneindex] RMSE &lt;- as.data.frame( matrix(NA, nrow = 10, ncol = 2) ) #rep(NULL, c(10,2)) lrfit1 &lt;- train(y~poly(x,degree=1), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[1,1] &lt;- lrfit1$results$RMSE predictedValues1&lt;-predict(lrfit1, newdata = data.frame(x=ytest) ) RMSE[1,2] &lt;- sqrt( mean( (predictedValues1-ytest)^2 ) ) lrfit2 &lt;- train(y~poly(x,degree=2), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[2,1] &lt;- lrfit2$results$RMSE predictedValues2&lt;-predict(lrfit2, newdata = data.frame(x=xtest) ) RMSE[2,2] &lt;- sqrt( mean( (predictedValues2-ytest)^2 ) ) lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[3,1] &lt;- lrfit3$results$RMSE predictedValues3&lt;-predict(lrfit3, newdata = data.frame(x=xtest) ) RMSE[3,2] &lt;- sqrt( mean( (predictedValues3-ytest)^2 ) ) lrfit4 &lt;- train(y~poly(x,degree=4), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[4,1] &lt;- lrfit4$results$RMSE predictedValues4&lt;-predict(lrfit4, newdata = data.frame(x=xtest) ) RMSE[4,2] &lt;- sqrt( mean( (predictedValues4-ytest)^2 ) ) lrfit5 &lt;- train(y~poly(x,degree=5), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[5,1] &lt;- lrfit5$results$RMSE predictedValues5&lt;-predict(lrfit5, newdata = data.frame(x=xtest) ) RMSE[5,2] &lt;- sqrt( mean( (predictedValues5-ytest)^2 ) ) lrfit6 &lt;- train(y~poly(x,degree=6), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[6,1] &lt;- lrfit6$results$RMSE predictedValues6&lt;-predict(lrfit6, newdata = data.frame(x=xtest) ) RMSE[6,2] &lt;- sqrt( mean( (predictedValues6-ytest)^2 ) ) lrfit7 &lt;- train(y~poly(x,degree=7), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[7,1] &lt;- lrfit7$results$RMSE predictedValues7&lt;-predict(lrfit7, newdata = data.frame(x=xtest) ) RMSE[7,2] &lt;- sqrt( mean( (predictedValues7-ytest)^2 ) ) lrfit8 &lt;- train(y~poly(x,degree=8), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[8,1] &lt;- lrfit8$results$RMSE predictedValues8&lt;-predict(lrfit8, newdata = data.frame(x=xtest) ) RMSE[8,2] &lt;- sqrt( mean( (predictedValues8-ytest)^2 ) ) lrfit9 &lt;- train(y~poly(x,degree=9), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[9,1] &lt;- lrfit9$results$RMSE predictedValues9&lt;-predict(lrfit9, newdata = data.frame(x=xtest) ) RMSE[9,2] &lt;- sqrt( mean( (predictedValues9-ytest)^2 ) ) lrfit10 &lt;- train(y~poly(x,degree=15), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[10,1] &lt;- lrfit10$results$RMSE predictedValues10&lt;-predict(lrfit10, newdata = data.frame(x=xtest) ) RMSE[10,2] &lt;- sqrt( mean( (predictedValues10-ytest)^2 ) ) We can now look at the RMSE in the held-out data as a function of polynomial degree: ggplot(data=RMSE, aes(x=c(1,2,3,4,5,6,7,8,9,10), y=V2)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;steelblue&quot;) + theme_bw() Let's plot the supposed best model: lrfit3 &lt;- train(y~poly(x,degree=8), data=data.frame(x=D[1:72,1],y=D[1:72,geneindex]), method = &quot;lm&quot;) lrfit4 &lt;- train(y~poly(x,degree=8), data=data.frame(x=D[97:168,1],y=D[97:168,geneindex]), method = &quot;lm&quot;) newX &lt;- seq(0,48,by=0.5) predictedValues&lt;-predict(lrfit3,newdata = data.frame(x=newX) ) predictedValues2 &lt;- predict(lrfit4, newdata = data.frame(x=newX)) ggplot(D, aes(x = Time, y = AT2G28890, colour = factor(Class))) + geom_point(size=2.5) + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) + geom_line(color=&#39;blue&#39;,data = data.frame(x=newX,y=predictedValues2), aes(x=x, y=y)) + geom_line(color=&#39;red&#39;,data = data.frame(x=newX,y=predictedValues), aes(x=x, y=y)) + theme_bw() In the above plots, we can see the decrease in RMSE as model complexity increases, and get a hint that it's beginning to increase as models become too complex, but it's not exactly obvious. One issue is that we chose our test set as being one of the four time series (trained on the first \\(3\\)), our test locations were at the same points as the input training time series, making it harder to distinguish between models. An alternative approach would be to make a training/test set split over particular time points, for example we might want to pick the last \\(3\\) time points to be our test set. xtrain &lt;- D[-which(D$Time %in% c(44,46,48) ),1] ytrain &lt;- D[-which(D$Time %in% c(44,46,48) ),geneindex] xtest &lt;- D[which(D$Time %in% c(44,46,48) ),1] ytest &lt;- D[which(D$Time %in% c(44,46,48) ),geneindex] RMSE &lt;- as.data.frame( matrix(NA, nrow = 10, ncol = 2) ) #rep(NULL, c(10,2)) lrfit1 &lt;- train(y~poly(x,degree=1), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[1,1] &lt;- lrfit1$results$RMSE predictedValues1&lt;-predict(lrfit1, newdata = data.frame(x=ytest) ) RMSE[1,2] &lt;- sqrt( mean( (predictedValues1-ytest)^2 ) ) lrfit2 &lt;- train(y~poly(x,degree=2), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[2,1] &lt;- lrfit2$results$RMSE predictedValues2&lt;-predict(lrfit2, newdata = data.frame(x=xtest) ) RMSE[2,2] &lt;- sqrt( mean( (predictedValues2-ytest)^2 ) ) lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[3,1] &lt;- lrfit3$results$RMSE predictedValues3&lt;-predict(lrfit3, newdata = data.frame(x=xtest) ) RMSE[3,2] &lt;- sqrt( mean( (predictedValues3-ytest)^2 ) ) lrfit4 &lt;- train(y~poly(x,degree=4), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[4,1] &lt;- lrfit4$results$RMSE predictedValues4&lt;-predict(lrfit4, newdata = data.frame(x=xtest) ) RMSE[4,2] &lt;- sqrt( mean( (predictedValues4-ytest)^2 ) ) lrfit5 &lt;- train(y~poly(x,degree=5), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[5,1] &lt;- lrfit5$results$RMSE predictedValues5&lt;-predict(lrfit5, newdata = data.frame(x=xtest) ) RMSE[5,2] &lt;- sqrt( mean( (predictedValues5-ytest)^2 ) ) lrfit6 &lt;- train(y~poly(x,degree=6), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[6,1] &lt;- lrfit6$results$RMSE predictedValues6&lt;-predict(lrfit6, newdata = data.frame(x=xtest) ) RMSE[6,2] &lt;- sqrt( mean( (predictedValues6-ytest)^2 ) ) lrfit7 &lt;- train(y~poly(x,degree=7), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[7,1] &lt;- lrfit7$results$RMSE predictedValues7&lt;-predict(lrfit7, newdata = data.frame(x=xtest) ) RMSE[7,2] &lt;- sqrt( mean( (predictedValues7-ytest)^2 ) ) lrfit8 &lt;- train(y~poly(x,degree=8), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[8,1] &lt;- lrfit8$results$RMSE predictedValues8&lt;-predict(lrfit8, newdata = data.frame(x=xtest) ) RMSE[8,2] &lt;- sqrt( mean( (predictedValues8-ytest)^2 ) ) lrfit9 &lt;- train(y~poly(x,degree=9), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[9,1] &lt;- lrfit9$results$RMSE predictedValues9&lt;-predict(lrfit9, newdata = data.frame(x=xtest) ) RMSE[9,2] &lt;- sqrt( mean( (predictedValues9-ytest)^2 ) ) lrfit10 &lt;- train(y~poly(x,degree=15), data=data.frame(x=xtrain,y=ytrain), method = &quot;lm&quot;) RMSE[10,1] &lt;- lrfit10$results$RMSE predictedValues10&lt;-predict(lrfit10, newdata = data.frame(x=xtest) ) RMSE[10,2] &lt;- sqrt( mean( (predictedValues10-ytest)^2 ) ) ggplot(data=RMSE, aes(x=c(1,2,3,4,5,6,7,8,9,10), y=V2)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;steelblue&quot;) + theme_bw() Now things become a little more obvious. In this example polynomial of degree \\(4\\) is the best fit. We can plot the best models: lrfit3 &lt;- train(y~poly(x,degree=4), data=data.frame(x=D[1:72,1],y=D[1:72,geneindex]), method = &quot;lm&quot;) lrfit4 &lt;- train(y~poly(x,degree=4), data=data.frame(x=D[97:168,1],y=D[97:168,geneindex]), method = &quot;lm&quot;) newX &lt;- seq(0,48,by=0.5) predictedValues&lt;-predict(lrfit3,newdata = data.frame(x=newX) ) predictedValues2 &lt;- predict(lrfit4, newdata = data.frame(x=newX)) ggplot(D, aes(x = Time, y = AT2G28890, colour = factor(Class))) + geom_point(size=2.5) + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) + geom_line(color=&#39;blue&#39;,data = data.frame(x=newX,y=predictedValues2), aes(x=x, y=y)) + geom_line(color=&#39;red&#39;,data = data.frame(x=newX,y=predictedValues), aes(x=x, y=y)) + theme_bw() Which seems to suggest that, for this dataset, a more complex model is better than the simple linear regression we began with, which is in line with our intuition of this gene being a circadian one. In practice, high-order polynomials are not ideal models for real world data, and we will instead move to more flexible approaches to regression including decision trees, neural networks. Nevertheless, the principle of using held out data to select a good model remains true in these cases. And now that we have a understanding of regression in the context of machine learning, we can easily incroporate more complex models (including nonlineaar regression) into our toolbox, and use these diverse approaches for a variey of means: for making predictions of continuous variables, for making decisions about future, and for extracing understanding about the nature of the dataset itself (model selection). Excerise 1.1: Think about how regression models can be used as a means for testing differential expression of time-series data. Hint: frame this as alternative hypothesis, the first where there is no differetial expression, the time series should be described by an identical model, whilst the second case, the idividual time series would require two independent models. Excercise 1.2: Given a set of time series, like our Arabidopsis dataset, think about how regression can be used to infer regulatory networks. 5.1.3 Logistic regression The type of linear regression models we've been using up to this point deal with real-valued observation data, \\(\\mathbf{y}\\), and are therefore not appropriate for classification. To deal with cases where \\(\\mathbf{y}\\) is a binary outcome, we instead have to think of different models, use different objective functions to optimise, and use different metrics to choose between competing models. Fortunately, however, much of the framework used for regression remains the same. Logistic regression is a model which can be used for data in which there is a general transition from one state to another as a function of the input variable e.g., where gene expression levels might predict a binary disease state, with lower levels indicating disease-free, and higher-levels indicating a diseased state. Logistic regression does not perform classification per se, but instead models the probability of a successful event (e.g., the probability that for a given expression the observation was in the diseased free state, \\(0\\), or diseased state \\(1\\)). As probability is a real-valued number (between \\(0\\) and \\(1\\)), technically this remains a form of regression. However, we can use logistic regression to make classifications by setting thresholds on those probabilities i.e., for prediction purposes we decide everything with \\(p\\ge 0.5\\) is a success (\\(1\\)), and everything below is a \\(0\\). Another way to think about linear regression is that we are fitting a linear model to the logit (natural log) of the log-odds ratio: \\(\\ln \\biggl{(}\\frac{p(x)}{1-p(x)}\\biggr{)} = c + m_1 x_1.\\) Although this model is not immediately intuitive, if we solve for \\(p(x)\\) we get: \\(p(x) = \\frac{1}{1+\\exp(-c - m_1 x_1)}\\). We have thus specified a function that indicates the probability of success for a given value of \\(x\\) e.g., \\(P(y=1|x)\\). In general can think of our data as a being a sample from a Bernoulli trial, and can therefore write down the likelihood for a set of observations \\({\\mathbf{X},\\mathbf{y}}\\): \\(\\mathcal{L}(c,m_1) = \\prod_{i=1}^n p(x_i)^{y_i} (1-p(x_i)^{1-y_i})\\). This is our objective function that we seek to maximise. Unlike linear regression, these models do not admit a closed form solution, but can be solved iteratively. The end result is the same, we find values \\((c,m_1)\\) that return the greatest value of \\(\\mathcal{L}(c,m_1)\\). Within {caret}, logistic regression can applied using the {glm} function. To illustate this we will again make use of our plant dataset. Recall that the third column represents a binary variable indicative of infection status. That is, indicating the population of the Botrytis cinerea pathogen based on detectable Botrytis tubulin. Thus, the value of this variable is \\(0\\) for the entirety of the control time series, and \\(0\\) for the earliest few time points of the infected time series, since Botrytis takes some time to proliferate and breach the plant cell walls. In the codde, below, we will use logistic regression to learn a set of markers capable of predicting infection status. To begin with, let's see if time is informative of infection status: ## Warning: package &#39;pROC&#39; was built under R version 3.5.2 ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess Here we have again split the data into a training and test set. We can calulate the probability that each datapoint in the test set belongs to class \\(0\\) or \\(1\\): prob &lt;- predict(mod_fit, newdata=data.frame(x = Dtest$Time, y = as.factor(Dtest$Infec)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Dtest$Infec)) In the above snippet we calculate the probability of each data point belonging to class 0 vs class 1, and using an incremented set of cutoffs use these to make a binary classification. To evaluate how well the algorithm has done, we can calculate a variety of summary statistics. For example for a given cutoff (say 0.5) we can calculate the number of true positives, true negatives, false positives, and false negatives. A useful summary is to plot the ROC curve (false positive rate versus true positive rate for all cutoffs) and calculate the area under that curve. For a perfect algorithm the area under this curve (AUC) will be equal to \\(1\\), whereas random assignment would give an area of \\(0.5\\). In the example below, we will calculate the AUC for a logistic regression model: perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) plot(perf) auc &lt;- performance(pred, measure = &quot;auc&quot;) auc &lt;- auc@y.values[[1]] auc ## [1] 0.6111111 Okay, so a score of \\(0.61\\) is certainly better than random, but not particularly good. This is perhaps not surprising, as half the time series (the control) is uninfected over the entirety of the time series, whilst in the second times series Botrytis is able to infect from around \\(8h\\) onward. The slightly better than random performance therefore arises due the slight bias in the number of instances of each class. Indeed, if we plot infection status vs time, we should be able to see why the model fails to be predictive. Let us see if AT2G28890 expression is informative: mod_fit &lt;- train(y ~ ., data=data.frame(x = Dtrain$AT2G28890, y = as.factor(Dtrain$Infec)), method=&quot;glm&quot;, family=&quot;binomial&quot;) prob &lt;- predict(mod_fit, newdata=data.frame(x = Dtest$AT2G28890, y = as.factor(Dtest$Infec)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Dtest$Infec)) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) plot(perf) auc &lt;- performance(pred, measure = &quot;auc&quot;) auc &lt;- auc@y.values[[1]] auc ## [1] 0.7795414 Both the ROC curve and the AUC score are much better. In the example below, we now regress infection status against individual gene expression levels for all genes in our set. The idea is to identify genes that have expression values indicative of Botrytis infection: marker genes. ggplot(data.frame(x=seq(4,165,1),y=aucscore[4:165]), aes(x = x, y = y)) + geom_point(size=2.5) + geom_point(color=&#39;blue&#39;) + geom_hline(aes(yintercept = 0.8)) + theme_bw() We note that, several genes in the list appear to have AUC scores much greater than \\(0.6\\). We can take a look at some of the genes with high predictive power: genenames[which(aucscore&gt;0.8)] ## [1] &quot;AT1G13030&quot; &quot;AT1G32230&quot; &quot;AT1G45145&quot; &quot;AT1G67170&quot; &quot;AT2G21380&quot; &quot;AT2G27480&quot; ## [7] &quot;AT2G35500&quot; &quot;AT2G44950&quot; &quot;AT3G02150&quot; &quot;AT3G09980&quot; &quot;AT3G11590&quot; &quot;AT3G13720&quot; ## [13] &quot;AT3G44720&quot; &quot;AT3G48150&quot; &quot;AT3G49570&quot; &quot;AT3G54170&quot; &quot;AT4G00710&quot; &quot;AT4G00980&quot; ## [19] &quot;AT4G01090&quot; &quot;AT4G02150&quot; &quot;AT4G19700&quot; &quot;AT4G26110&quot; &quot;AT4G26450&quot; &quot;AT4G28640&quot; ## [25] &quot;AT4G34710&quot; &quot;AT4G36970&quot; &quot;AT4G39050&quot; &quot;AT5G11980&quot; &quot;AT5G22630&quot; &quot;AT5G25070&quot; ## [31] &quot;AT5G50010&quot; &quot;AT5G56290&quot; &quot;AT5G57210&quot; &quot;AT5G59670&quot; &quot;AT5G66560&quot; Unsurprisingly, among these genes we see a variety whose proteins are known to be targeted by various pathogen effectors, and are therefore directly implicated in the immune response (Table 1). Gene Effector AT3G25710 ATR1_ASWA1 AT4G19700 ATR13_NOKS1 AT4G34710 ATR13_NOKS1 AT4G39050 ATR13_NOKS1 AT5G24660 ATR13_NOKS1 AT4G00710 AvrRpt2_Pto JL1065_CatalyticDead AT4G16380 HARXL44 AT2G45660 HARXL45 AT5G11980 HARXL73 AT2G35500 HARXLL445 AT1G67170 HARXLL470_WACO9 AT4G36970 HARXLL470_WACO9 AT5G56250 HARXLL470_WACO9 AT3G09980 HARXLL516_WACO9 AT5G50010 HARXLL60 AT3G44720 HARXLL73_2_WACO9 AT5G22630 HARXLL73_2_WACO9 AT5G43700 HopH1_Psy B728A Table 1: Genes predictive of infection status of Botrytis cinerea whose proteins are targeted by effectors of a variety of pathogens As always, let's take a look at what our model and the data look like. In this case we plot the training data labels and the fit from the logistic regression i.e., \\(p(\\mathbf{y}=1|\\mathbf{x})\\): bestpredictor &lt;- which(aucscore==max(aucscore))[1] best_mod_fit &lt;- train(y ~., data=data.frame(x = Dtrain[,bestpredictor], y = as.factor(Dtrain$Infec)), family=&quot;binomial&quot;, method=&quot;glm&quot;) xpred &lt;- seq(min(Dtest[,bestpredictor]),max(Dtest[,bestpredictor]),length=200) ypred &lt;- predict(best_mod_fit,newdata=data.frame(x = xpred),type=&quot;prob&quot;)[,2] Data_to_plot &lt;- data.frame(x = Dtest[,bestpredictor],y=Dtest[,3]) ggplot(Data_to_plot, aes(x = x, y = y, colour = factor(y) )) + geom_point(size=2.5) + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) + geom_line(color=&#39;red&#39;,data = data.frame(x=xpred,y=ypred), aes(x=x, y=y)) + geom_hline(aes(yintercept = 0.5)) + theme_bw() We can see from this plot that the level of AT2G21380 appears to be highly predictive of infection status. When AT2G21380 is low, its almost certain that the Botrytis cinerea has gained a foothold; whether this is causal or not, we cannot say, but it is certainly a good marker and a good starting point for further testing. We could also make predictions using more than one variable. This might be useful, for example to figure out if there are any combinations of genes that together contain additional information. In the snippet of code we search for all combinations with AT2G21380: aucscore2 &lt;- matrix(0, 1, 165) for (i in seq(4,165)){ mod_fit &lt;- train(y ~ ., data=data.frame(x = Dtrain[,unique(c(bestpredictor,i))], y = as.factor(Dtrain$Infec)), method=&quot;glm&quot;, family=&quot;binomial&quot;) prob &lt;- predict(mod_fit, newdata=data.frame(x = Dtest[,unique(c(bestpredictor,i))], y = as.factor(Dtest$Infec)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Dtest$Infec)) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) aucscore2[i] &lt;- auc@y.values[[1]] } ggplot(data.frame(x=seq(4,165,1),y=aucscore2[4:165]), aes(x = x, y = y)) + geom_point(size=2.5) + geom_point(color=&#39;blue&#39;) + geom_hline(aes(yintercept = max(aucscore) )) + theme_bw() This looks promising - there are a few combinations that allow us to make even better predictions. However, we have made one mistake! If we are to do this properly, we must ensure that we are making decisions on a dataset that has not been seen by the model. In the above case, this would not strictly be true, as we have selected our &quot;best gene&quot; using the test data. Thus to do this properly we would have to either look at pairwise combinations at an earlier step (before we picked our best gene), or have access to a third batch of datapoints on which to do the selection. Indeed it is quite common to see datasets broken down into training, evaluation, and test sets, with the second set used to pick between several competing models. Luckily I have just such a dataset that I have been holding in reserve: Deval &lt;- read.csv(file = &quot;data/Arabidopsis/Arabidopsis_Botrytis_transpose_3.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) aucscore2 &lt;- matrix(0, 1, 165) for (i in seq(4,165)){ mod_fit &lt;- train(y ~ ., data=data.frame(x = Dtrain[,unique(c(bestpredictor,i))], y = as.factor(Dtrain$Infec)), method=&quot;glm&quot;, family=&quot;binomial&quot;) prob &lt;- predict(mod_fit, newdata=data.frame(x = Deval[,unique(c(bestpredictor,i))], y = as.factor(Deval$Infec)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Deval$Infec)) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) aucscore2[i] &lt;- auc@y.values[[1]] } ggplot(data.frame(x=seq(4,165,1),y=aucscore2[4:165]), aes(x = x, y = y)) + geom_point(size=2.5) + geom_point(color=&#39;blue&#39;) + geom_vline(aes(xintercept = 28 )) + theme_bw() Here we indicate by a vertical line the index corresponding to the basal model (i.e, AT2G21380 only) and can see there are several combinatorial models that allow us to more accurately predict infection status. 5.2 Using regression approaches to infer 3D gene expression patterns in marmoset embryos In our recent paper (Bergmann et al. 2022) we combined laser capture microdissection (LCM) with RNA-sequencing and immunofluorescent staining to generate 3D transcriptional reconstructions of early post-implantation marmoset embryos. Here, adjacent sections were used to build a 3D model of the embryo (see 5.2). The 3D position of each LCM within this reconstructed embryo was retained, allowing an interpolation of expression patterns across the embryo using regression (albeit a nonlinear form of rgression). Raw sequencing data is available from ArrayExpress under accession numbers E-MTAB-9367 and E-MTAB-9349. Figure 5.2: 3D representations of primate embryos at Carnegie stages 5, 6, and 7. Laser capture microdissection allowed comprehesnive RNA-sequencing at near single cell level whislt retaining 3D-sptial information In the exammpels below we will take a look at the 3D models and use simple linear regression to investigate anterior-posterior gradients in the embryonic disc. First we will load the 3D &quot;scaffolds&quot; for the embryonic disc and the amnion. This consists of a set of vetices and a set of indices that define faces of the objecct. This example scaffold was constructed from sequential sections of a Carnegie stage 6 (CS6) marmoset embryo. ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout We will also load in the 3D locations of the laser capture microdissection samples that we used to do RNA-seq and the gene-expression of those samples. For simplicity I've included a processed expression matrix for a handful of genes. D5 &lt;- read.table(&quot;data/Embryo/CS6Expression.csv&quot;,sep=&quot;,&quot;,header=T) As an illustrative example of regression we will look at the expression pattern differences in the embryonic disc versus the amnion. In this case we will do regression on embryonic disc and amnion seperately. ind1 &lt;- which(D5$Lineage==&quot;EmDisc_CS6&quot;) ind2 &lt;- which(D5$Lineage==&quot;Am_CS6&quot;) In the snippet of code below we use plotly to plot the scaffold tissues We can also visualise where the LCM samples are within the embryo: Okay so now we can do some regression. We first infer a model using the embryonic disc samples, and then infer a value over the full scaffold region: lrfit &lt;- train(expr~x+y+z, data=data.frame(expr=D5$SOX2[ind1],x=D5$X[ind1],y=D5$Y[ind1],z=D5$Z[ind1]), method = &quot;lm&quot;) newX &lt;- data.frame(x=D1$V1,y=D1$V2,z=D1$V3) predictedValues&lt;-predict(lrfit, newX) We can visualise the interpolated values of \\(SOX2\\) on the embryonic disc: We could do the same for the expression level of T and other genes. In the snippets of code below we have opted not to evaluate the code to keep the workbook size down, but these may still be run in your R session. lrfit &lt;- train(expr~x+y+z, data=data.frame(expr=D5$T[ind1],x=D5$X[ind1],y=D5$Y[ind1],z=D5$Z[ind1]), method = &quot;lm&quot;) newX &lt;- data.frame(x=D1$V1,y=D1$V2,z=D1$V3) predictedValues&lt;-predict(lrfit, newX) plot_ly(x = D1$V1, y = D1$V2, z = D1$V3, i = c(D2$V1-1),j=c(D2$V2-1),k=c(D2$V3-1), intensity = predictedValues, colorscale = list(c(0,&#39;red&#39;), c(0.33,&#39;orange&#39;), c(0.66, &#39;yellow&#39;), c(1, &#39;green&#39;)), type = &quot;mesh3d&quot;) %&gt;% layout(scene = list(xaxis = ax, yaxis = ax, zaxis = ax), font = list(color=&#39;#FFFFFF&#39;)) Here we visualise expression of SOX2 on both the embryonic disc and amnion, in order to see tissue specific biases in gene expression. lrfit1 &lt;- train(expr~x+y+z, data=data.frame(expr=D5$SOX2[ind1],x=D5$X[ind1],y=D5$Y[ind1],z=D5$Z[ind1]), method = &quot;lm&quot;) lrfit2 &lt;- train(expr~x+y+z, data=data.frame(expr=D5$SOX2[ind2],x=D5$X[ind2],y=D5$Y[ind2],z=D5$Z[ind2]), method = &quot;lm&quot;) newX &lt;- data.frame(x=D1$V1,y=D1$V2,z=D1$V3) predictedValues1&lt;-predict(lrfit1, newX) predictedValues2&lt;-predict(lrfit2, newX) maxE &lt;- max(c(predictedValues1,predictedValues2)) minE &lt;- min(c(predictedValues1,predictedValues2)) #NB this is a hack. Plot_ly rescales internally so to make sure the two barplots are on the same scale we need to make sure they have the same limits. The final two colour points are not used directly predictedValues1&lt;- c(predictedValues1,minE,maxE) predictedValues2&lt;- c(predictedValues2,minE,maxE) p &lt;- plot_ly(x = D1$V1, y = D1$V2, z = D1$V3, i = c(D2$V1-1),j=c(D2$V2-1),k=c(D2$V3-1), intensity = c(predictedValues1), colorscale = list(c(0,&#39;red&#39;), c(0.33,&#39;orange&#39;), c(0.66, &#39;yellow&#39;), c(1, &#39;green&#39;)), type = &quot;mesh3d&quot;) p &lt;- p %&gt;% add_trace(x = D1$V1, y = D1$V2, z = D1$V3, i = c(D3$V1-1),j=c(D3$V2-1),k=c(D3$V3-1), intensity = c(predictedValues2), colorscale = list(c(0,&#39;red&#39;), c(0.33,&#39;orange&#39;), c(0.66, &#39;yellow&#39;), c(1, &#39;green&#39;)), type = &quot;mesh3d&quot;) %&gt;% layout(scene = list(xaxis = ax, yaxis = ax, zaxis = ax), font = list(color=&#39;#000000&#39;)) p Coversely we can look at an amnion marker VTCN1: lrfit1 &lt;- train(expr~x+y+z, data=data.frame(expr=D5$VTCN1[ind1],x=D5$X[ind1],y=D5$Y[ind1],z=D5$Z[ind1]), method = &quot;lm&quot;) lrfit2 &lt;- train(expr~x+y+z, data=data.frame(expr=D5$VTCN1[ind2],x=D5$X[ind2],y=D5$Y[ind2],z=D5$Z[ind2]), method = &quot;lm&quot;) newX &lt;- data.frame(x=D1$V1,y=D1$V2,z=D1$V3) predictedValues1&lt;-predict(lrfit1, newX) predictedValues2&lt;-predict(lrfit2, newX) maxE &lt;- max(c(predictedValues1,predictedValues2)) minE &lt;- min(c(predictedValues1,predictedValues2)) #NB this is a hack. Plot_ly rescales internally so to make sure the two barplots are on the same scale we need to make sure they have the same limits. The final two colour points are not used directly predictedValues1&lt;- c(predictedValues1,minE,maxE) predictedValues2&lt;- c(predictedValues2,minE,maxE) p &lt;- plot_ly(x = D1$V1, y = D1$V2, z = D1$V3, i = c(D2$V1-1),j=c(D2$V2-1),k=c(D2$V3-1), intensity = c(predictedValues1), colorscale = list(c(0,&#39;red&#39;), c(0.33,&#39;orange&#39;), c(0.66, &#39;yellow&#39;), c(1, &#39;green&#39;)), type = &quot;mesh3d&quot;) p &lt;- p %&gt;% add_trace(x = D1$V1, y = D1$V2, z = D1$V3, i = c(D3$V1-1),j=c(D3$V2-1),k=c(D3$V3-1), intensity = c(predictedValues2), colorscale = list(c(0,&#39;red&#39;), c(0.33,&#39;orange&#39;), c(0.66, &#39;yellow&#39;), c(1, &#39;green&#39;)), type = &quot;mesh3d&quot;) %&gt;% layout(scene = list(xaxis = ax, yaxis = ax, zaxis = ax), font = list(color=&#39;#000000&#39;)) p Whilst in these examples we have not used a rigorous treatment of the data, they should illustrate the power of regression and some of the examples where they might be useuful in more contemporary settings. Indeed in our paper we make explicit use of different regression models (Gaussian processes) to statistically identify tissue that exhibit strong spatial biases. 5.3 Resources A variety of examples using {caret} to perform regression and classification have been implemented here. For those that want to start their own reading on nonlinear regression, a good stating point is Rasmussen and William's book on Gaussian processes. Be warned, it will contain a lot more maths than this course. We also have an introductory section in this work book. A brief primer on Gaussian Processes can also be found in appendix 9. ======= ## Exercises Solutions to exercises can be found in appendix 7. References "],
["mlnn.html", "6 Deep Learning 6.1 Multilayer Neural Networks 6.2 Convolutional neural networks 6.3 Multiclass prediction 6.4 CNNs for Motif analysis 6.5 Further reading", " 6 Deep Learning 6.1 Multilayer Neural Networks Neural networks are increasingly used to address a variety of complex problems in biology under the umbrella of deep learning. This umbrella contains a diverse range of techniques, tools, and heursitics, that have collectively been used to address everything from image analysis (Chamier et al. 2021) to the data avalanch of modern genomics (Angermueller and Stegle 2016,Mohammad Lotfollahi (2019),Zhang et al. (2022)); from protein structure prerdiction (Jumper et al. 2021) to drug perrturbation prediction and discovery (Gómez-Bombarelli et al. 2018,Rampášek et al. (2019)). These techniques include densely connected networks, convolutional neural networks (CNN), autoencoders (AE), and adversarial neural networks (ANN), as well as more recent developments such as diffusion models. In this chapter we will explore some of the basics of deep learning on a practical level. We will first learn how to construct a simple densly connected neural networks within R using {keras}, and use these networks for regression. Later we will try our hand at image classification using a set of images taken from the animated TV series Rick and Morty. For those unfamiliar with Rick and Morty, the series revolves around the adventures of Rick Sanchez, an alcoholic scientist, and his neurotic grandson, Morty Smith. Although many scientists aspire to be like Rick, they're usually more of a Jerry. Our motivating goal in the latter sections will be to develop an image classification algorithm capable of telling if any given image contains Rick or not. Finally, we will adapt these techniques to identify DNA motifs from ChIP-sequencing data. The main take home message from this section are: It's important to look at the data. It's probably just as important to have a clear question in mind. A clear question should precede the data generation (or data assembly) step. There are a limitless variety of architectures that can be built into a neural networks. Picking one to use is often arbitrary or at best empirically-motivated by previous works. Having a clear idea of the limitations of the data and having a clear question can help pin this down. Some apprroaches and architectures are particularly suited to specific tasks. 6.1.1 Regression with Keras We will first build a multilayer densely connected Neural Network (NN) to perform regression. A user friendly package for neural networks is available via keras, an application programming interface (API) written in Python, which uses either theano or tensorflow as a back-end. An R interface for keras is available in the form of keras/tensorflow for R. We will first install (or import) the relevant packages (see section 3 for further details about installation or follow the installation instructions here). In my case, I have installed Tensorflow within Python3.9, and can set R to call this version using reticulate. We will also load a few other packages for general plotting and data manipulation. ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union ## Warning: package &#39;jpeg&#39; was built under R version 3.5.2 ## Warning: package &#39;reticulate&#39; was built under R version 3.5.2 The task of building -- and using -- a densly connected neural network moreorless follows the same recipe as for other ML models: We first generate or assemble an appropriate dataset, which will be split into training/test/validation sets. We next pick an appropriate method to address a specific question. This consists of choosing a particular architecture, initialising some parameters, and setting others. Finally, we pick an appropriate objective function we wish to optimise with respect to the parameters of the model. Once optimised, we can use the model to make predictions or use the results to intepret aspects of the system we're studying. As a motivvating example, we first aim to build a neural network to approximate the square root of a number. We will have to generate a synthetic dataset: for the training set we generate two arrays, an input array, containing a random set of numbers sampled from a given interval (e.g. between \\(0\\) and \\(100\\)), and an output array, containing the square roots of those numbers, which we have numerically calculated. A similar set of data will be independently generated for the validation set: set.seed(12345) tdims &lt;- 50 #Number of samples to generate #We first generate random locations from an interval for the training set and store them as an array, then numerically calculate the square root at these locations (again storing as an array) trainingX &lt;- array(0, dim=c(tdims,1)) #Store data as an array (as required by Keras) trainingX[1:tdims,1] &lt;- runif(tdims, min=0, max=100) #Generate random x in range 0 to 100 trainingY &lt;- array(0, dim=c(tdims,1)) trainingY[1:tdims,1] &lt;- sqrt(trainingX[1:tdims,1]) #Now do the same but for a independently generated test set testingX &lt;- array(0, dim=c(tdims,1)) #Store as arrays testingX[1:tdims,1] &lt;- runif(tdims, min=0, max=100) testingY &lt;- array(0, dim=c(tdims,1)) testingY[1:tdims,1] &lt;- sqrt(testingX[1:tdims,1]) Here we have a 1D (scalar) input, and a 1D (scalar) output, and need to construct a multi-layer neural network to approximate the function that maps form one to the other, in a way that hopefully generalises. Now we need to construct the actual neural network that will do this. Keras has an simple and intuitive way of specifying layers of a neural network, and the R wrapper for keras makes good use of this (see [here for documentation]{https://tensorflow.rstudio.com/}). We first initialise the model: model &lt;- keras_model_sequential() This tells keras that we're using the Sequential API i.e., a simple architecture in which the first layer will be connected to the second, the second to the third and so forth, which distinguishes it from more complex networks possible using the Model API. Once we've specified this sequential model, we can start adding layers to the neural network one by one. When specifying the first layer of the network, we must also include the dimension of the input data. A standard layer of neurons, can be specified using the {layer_dense} function. When adding a layer we also need to specify the activation function to the next level. This can be done via {activation}. Early on step functions annd sigmoid activation funnctions were common, but were gradually replaced by relu activations, mostly for empircal performence reasons. So to add a layer of \\(100\\) nodes with a Rectified Linear Unit (relu) activation, our code would look something like: model &lt;- keras_model_sequential() %&gt;% layer_dense(input_shape = c(1),units = 100, activation = &quot;relu&quot;) This represets our first hidden layer. The various parameters of the model, such as the weights of the individual connections and biases, will be initialised by default, although if we were interested we could select from a number of differe schemes manually or even encode our own custom initialisation. In the default setting, the connection weights are initialised as a uniform random variables over a particular range, with the biases set to zero. We could add a second hidden layer of \\(120\\) neurons (with relu activation and randomly initialised parameters), so that our code would be: model &lt;- keras_model_sequential() %&gt;% layer_dense(input_shape = c(1), units = 100, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 120, activation = &quot;relu&quot;) Finally, we add the output neuron(s). The number of output neurons depends on our task: it should match the size of the output we're aiming to predict. In this section we have just one output, a scalar representing the square root of the input, so will have a {layer_dense(1)} output. Unlike the earlier layers, the final activation function will depend on the nature of our data, and should be appropriately set according to the task. If, for example, we're doing regression, in which the output variable could feasibly take any real valued number, it would be inappropriate to set a sigmoid activation, which would constrain our possible inference to lie between \\(0\\) and \\(1\\). Instead we might instead specify a {linear} activation function: model &lt;- keras_model_sequential() %&gt;% layer_dense(input_shape = c(1),units = 100, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 120, activation = &quot;relu&quot;) %&gt;% layer_dense(1, activation = &quot;linear&quot;) That's it. Simple! Our model is now complete; we have specified a multi-layered neural network for regression consisting of two hidden layers of width \\(100\\) and \\(120\\) respectively. We can print a summary of the network, to visualise how many parameters it has via: summary(model) In total, this model contans \\(12,441\\) parameters, many more parameters than we're used to for other machine learning models. However, in terms of size and number of parameters, this is a tiny model: some of the bigger models out there have already surpassed [\\(500\\) billion parameters]{https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/}. Before we use our model to perform inference, we must first compile it. In this case we need to specify three things: A loss function, which specifies the objective function that the we will try to minimise with respect to these \\(12,441\\) parameters. A number of existing loss functions are built into keras, including the mean squared error (mean_squared_error) for regression, binary cross entrophy for binary classification (binary_crossentropy), and categorical cross entropy (categorical_crossentropy), which is used for categorical classification. Since we are dealing with regression, we will stick with the mean squared error. An optimiser, which determines how the loss function is optimised. Possible examples include stochastic gradient descent ({sgd}), root mean square propagation ({rmsprop}), and Adam ({adam}). We will generally use Adam within this chapter. A list of metrics to return. These do not directly impact the optimisation, but are additional summary statistics that keras evaluates and prints out, which can come in very handy when intepreting or summarising the results. For classification, a good choice would be accuracy ({binary_accuracy} or {categorical_accuracy}) whilst for regression we could print out various metrics including the root mean square error, mean absolute error, and so forth. We compile our model using {compile}: model %&gt;% compile(loss = &quot;mse&quot;, optimizer = &quot;adam&quot;, metrics = &quot;mse&quot;) Finally the model is fitted to the data. When doing so we should additionally specify the validation set (if we have one), the batch size, and the number of epochs. Recall batch size is the number of training examples we draw at once for one forward/backward pass, and one epoch is one forward pass and one backward pass through all batches that constitute the complete training dataset. Our complete code would then look like the sippet below. model &lt;- keras_model_sequential() %&gt;% layer_dense(input_shape = c(1),units = 100, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 120, activation = &quot;relu&quot;) %&gt;% layer_dense(1, activation = &quot;linear&quot;) ## Loaded Tensorflow version 2.7.0 model %&gt;% compile(loss = &quot;mse&quot;, optimizer = &quot;adam&quot;, metrics = &quot;mse&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainingX, y = trainingY, validation_data = list(testingX, testingY), epochs = 100, verbose = 2) Alternatively to above, rather than specify the training and validation data sets manually, we could use validation_split to specify the fraction of data to be used as the validation set. This can be useful when dealing with larger datasets. Generally in this section we have very small datasets and can load all the input and output data into memory, but may not be possible if dealing with images or large genomic data. Instead we could specify validation_split along with flow_from_directory to read small batches directly from a directory. We can see that the mean square error rapidly decreases (from approx. 16 at epoch 3 to around 0.8 towards the end). Precicesly how the objective funciton is optimised is based on back propogation (you can read more on this [here]{https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd}, [here]{https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8}, and [here]{https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c}). As always, let's take a look at the actual results, rather than rely on summary metrics. To make predictions using the model we can use the {predict} function: xstar &lt;- seq(0,200,by=0.5) forecastY &lt;- model %&gt;% predict(xstar) ggplot(data.frame(x=xstar,y=forecastY ), aes(x = x, y = y)) + geom_line(size = 1) + geom_point(color=&#39;blue&#39;) + geom_line(color=&#39;red&#39;,size = 1, data = data.frame(x=xstar,y=sqrt(xstar)), aes(x=x, y=y)) + theme_bw() Okay, so it's not particularly good. However, we didn't use a large training set and there are a few things we can do to try to optimise the network. Another important point is that we didn't use the best network for prediction (the one with the best validation set error). By default when we call prediction functions we tend to use whatever the final network was during our training: if we ran for \\(10\\) epochs, it would be the network we had at the end of epoch \\(10\\). We can add a callback to save the best model, which would then look something like: model &lt;- keras_model_sequential() %&gt;% layer_dense(input_shape = c(1),units = 100, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 120, activation = &quot;relu&quot;) %&gt;% layer_dense(1, activation = &quot;linear&quot;) model %&gt;% compile(loss = &quot;mse&quot;, optimizer = &quot;adam&quot;, metrics = &quot;mse&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/densemodel.h5&#39;, save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_mse&quot;, verbose = 0) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainingX, y = trainingY, validation_data = list(testingX, testingY), epochs = 100, verbose = 0, callbacks = list(cp_callback)) This optimised model can be loaded in and used for prediction: model = load_model_hdf5(&#39;data/RickandMorty/data/models/densemodel.h5&#39;) xstar &lt;- seq(0,200,by=0.5) forecastY &lt;- model %&gt;% predict(xstar) ggplot(data.frame(x=xstar,y=forecastY ), aes(x = x, y = y)) + geom_line(size = 1) + geom_point(color=&#39;blue&#39;) + geom_line(color=&#39;red&#39;,size = 1, data = data.frame(x=xstar,y=sqrt(xstar)), aes(x=x, y=y)) + theme_bw() We can try varying a few other aspects of the network to get an idea of how NNs behave. For example, first try increasing the training set size. Try adding or removing layers, and varying layer widths. Another thing thing that can be varied is the final layer activation. The [keras manual]{https://keras.io/api/layers/activations/} should provide a useful resource to explore what options are available. In the snippet of code below we use a comparatively large dataset, with the input data generated in the region \\([0,80]\\) and \\([120,200]\\). By ensuring there is no input data generated in the range \\((80,120)\\) it is much easier to see if the moodel is extrapolating over locations with no data. tdims &lt;- 5000 #Number of samples to generate trainingX &lt;- array(0, dim=c(tdims,1)) #Store data as an array (required by Keras) trainingX[1:tdims,1] &lt;- c( runif(tdims/2, min=0, max=80), runif(tdims/2, min=120, max=200) ) trainingY &lt;- array(0, dim=c(tdims,1)) trainingY[1:tdims,1] &lt;- sqrt(trainingX[1:tdims,1]) #Now do the same but for a independently generated test set testingX &lt;- array(0, dim=c(tdims,1)) #Store as arrays testingX[1:tdims,1] &lt;- runif(tdims, min=0, max=200) testingY &lt;- array(0, dim=c(tdims,1)) testingY[1:tdims,1] &lt;- sqrt(testingX[1:tdims,1]) model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(1)) %&gt;% layer_dense(units = 10, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20, activation = &quot;relu&quot;) %&gt;% layer_dense(1, activation = &quot;linear&quot;) model %&gt;% compile(loss = &quot;mse&quot;, optimizer = &quot;adam&quot;, metrics = &quot;mse&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/densemodel.h5&#39;, save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_mse&quot;, verbose = 0) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainingX, y = trainingY, validation_data = list(testingX, testingY), epochs = 100, verbose = 0, callbacks = list(cp_callback)) model = load_model_hdf5(&#39;data/RickandMorty/data/models/densemodel.h5&#39;) xstar &lt;- seq(0,250,by=0.5) forecastY &lt;- model %&gt;% predict(xstar) dataLM &lt;- data.frame(x=trainingX,y=trainingY) lrfit &lt;- lm(data = dataLM, y~x) predictedValues&lt;-predict.lm(lrfit, newdata = data.frame(x=xstar) ) ggplot(data.frame(x=xstar,y=forecastY ), aes(x = x, y = y)) + geom_line(size = 1) + geom_point(color=&#39;blue&#39;) + geom_line(color=&#39;red&#39;,size = 1, data = data.frame(x=xstar,y=predictedValues), aes(x=x, y=y)) + geom_line(color=&#39;green&#39;,size = 1, data = data.frame(x=xstar,y=sqrt(xstar)), aes(x=x, y=y)) + theme_bw() Let's take a closer look over the range where we didn't have any training data: model = load_model_hdf5(&#39;data/RickandMorty/data/models/densemodel.h5&#39;) xstar &lt;- seq(60,140,by=0.5) forecastY &lt;- model %&gt;% predict(xstar) dataLM &lt;- data.frame(x=trainingX,y=trainingY) lrfit &lt;- lm(data = dataLM, y~x) predictedValues&lt;-predict.lm(lrfit, newdata = data.frame(x=xstar) ) ggplot(data.frame(x=xstar,y=forecastY ), aes(x = x, y = y)) + geom_line(size = 1) + geom_point(color=&#39;blue&#39;) + geom_line(color=&#39;red&#39;,size = 1, data = data.frame(x=xstar,y=predictedValues), aes(x=x, y=y)) + geom_line(color=&#39;green&#39;,size = 1, data = data.frame(x=xstar,y=sqrt(xstar)), aes(x=x, y=y)) + theme_bw() Not perfect, but better than a linear model, at least. So we have a model that has interpolated reasonably well. These types of models are pretty good at inference for a whole range of real world (nonlinear) functions for which there is a sufficient amount of data. Excercise 2.1: How could you modify the above code for inferece in a system where we have \\(5\\) input variables and \\(3\\) output variables? 6.1.2 Image classification with Rick and Morty We will now try to modify our network for image classification. As with any machine learning application, it's important to have both a question in mind (in this case &quot;can we identify images that contain Rick Sanchez&quot;), and understand the dataset(s) we're using. For training purposes I have downloaded several thousand random images of Rick Sanchez and several thousand images without Rick from the website Master of All Science. The image data can be found in the directory {data/RickandMorty/data/}, which we can load and plot using the {readJPEG} and {grid.raster} functions respectively. im &lt;- readJPEG(&quot;data/RickandMorty/altdata/AllRickImages/Rick_1.jpg&quot;) grid::grid.newpage() grid.raster(im, interpolate=FALSE, width = 0.5) We can use the funciton {dim(im)} to return the image dimensions. In this case each image is stored as a jpeg file, with a \\(90 \\times 160\\) pixel resolution with \\(3\\) colour channels (RGB). This loads into R as \\(160 \\times 90 \\times 3\\) array. We could start by converting the image to grey scale, reducing the dimensions of the input data. However, each channel will potentially carry novel information, so ideally we wish to retain all of the information. You can take a look at what information is present in the different channels by plotting them individually using e.g., {grid.raster(im[,,3], interpolate=FALSE)}. Whilst the difference is not so obvious here, we can imagine sitations where different channels could be dramamtically different, for example, when dealing with remote observation data from satellites, where we might have visible wavelengths alongside infrared and other wavelengths. Since we plan to retain the channel information, our input data is a tensor of dimension \\(90 \\times 160 \\times 3\\) i.e., height x width x channels. Note that this ordering is important, as keras expects this ordering (but be careful, as other packages can expect a different ordering). Before building a neural network we first have to load the data and construct a training, validation, and test set of data. Whilst the package we're using has the ability to specify this on the fly (flow_images_from_directory), for smaller datasets I prefer to manually seperate out training/test/validation sets, as it makes it a little easier debug when things go wrong. First load all Rick images and all not Rick images from their directory. We can get a list of all the Rick and not Rick images using {list.files}: files1 &lt;- list.files(path = &quot;data/RickandMorty/altdata/AllRickImages/&quot;, pattern = &quot;jpg&quot;) files2 &lt;- list.files(path = &quot;data/RickandMorty/altdata/AllMortyImages/&quot;, pattern = &quot;jpg&quot;) After loading the files we can see we have \\(2211\\) images of Rick and \\(3046\\) images of not Rick, for a total of \\(5257\\) imaages. Whilst this is a slight class imbalaance in the dataset (there are more not Rick images than Rick images) it is not dramatically so; in cases where there is extreme imbalance in the number of class observations we may have to do something extra, such as data augmentation, or assinging weights during training. We next preallocate an empty array to store these training images for the Rick and not Rick images (an array of dimension \\(5257 \\times 90 \\times 160 \\times 3\\)): allX &lt;- array(0, dim=c(length(files1)+length(files2),dim(im)[1],dim(im)[2],dim(im)[3])) We can load images using the {readJPEG} function: for (i in 1:length(files1)){ allX[i,1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/altdata/AllRickImages/&quot;, files1[i], sep=&quot;&quot;)) } Similarly, we can load the not Rick images and store in the same array: for (i in 1:length(files2)){ allX[i+length(files1),1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/altdata/AllMortyImages/&quot;, files2[i], sep=&quot;&quot;)) } Next we can construct the output variable (usually denoted Y). The nature of this depends on the data itself. In the simplest case we are looking to infer a Rick or not, which is a binary variable, with a zero (\\(0\\)) indicating Rick present, annd one (\\(1\\)) indicating no Rick (we could always flip this assignment round and have a \\(1\\) for Rick it shouldn't make a difference). The output can therefore be represented as a single (binary) output node. We can construct Y as a vector of length \\(5257\\) containing the classification for each of the images e.g., a \\(0\\) if the image is a Rick and \\(1\\) if it is not Rick. This is simple enough using the function {rbind}, as we know the first \\(2211\\) images were Rick (so create a vector of zeros of length 2211) and the second lot of images are not Rick (create a vector of ones of length 3046) and join them together using rbind. labels &lt;- rbind(matrix(0, length(files1), 1), #These are the Rick laabels matrix(1, length(files2), 1)) #These are the not Rick labels Next, we must now split our data in training sets, validation sets, and test sets. In fact I have already stored some seperate &quot;test&quot; set images in another folder that we will load in at the end, so here we only need to seperate images into training and validation sets. It's important to note that we shouldn't simply take the first \\(N\\) images for training with the remainder used for validation/testing, since this may introduce artefacts. For example, here we've loaded in all the Rick images in first, with the not Rick images loaded in second: if we took, say, the first \\(2000\\) images for training, we would be training with only Rick images, which makes our task impossible, and our algorithm will fail catastrophically. Although there are more elegant ways to shuffle data using {keras}, here we are going to randomly permute the data, and then take the first \\(4000\\) permuted images for training, with the remainder for validation (Note: it's crucial to permute the \\(Y\\) data in the same way). This gives us a \\(76%\\) to \\(24%\\) split for training to validation data. set.seed(12345) #Set random number generator for R aspects of the session vecInd &lt;- seq(1,length(files1)+length(files2)) #A vector of indexes of the length of the data trainInd &lt;- sample(vecInd)[1:4001] #Permute and take first 4000 training #Training set trainX &lt;- allX[trainInd, , , ] trainY &lt;- labels[trainInd, 1] #Val set valX &lt;- allX[-trainInd, , , ] valY &lt;- labels[-trainInd, 1] We are almost ready to begin building our neural networks. First we can try a few things to make sure our data has been processed correctly. For example, try manually plotting several of the images and seeing if the labels are correct. Manually print out the image matrix (not a visualisation of it): think about the range of the data, and whether it will need normalising. Finally we can check to see how many of each class is in the training and validation datasets (e.g., sum trainY/testY and compare to the length of trainY/testY). In this case there are \\(1681\\) images of Rick and \\(2320\\) images of not Rick in the training dataset. Again, whilst there is some slight class inbalance it is not terrible, so we don't need to perform data augmentation or assign weights to the different classes during training. 6.1.3 Rick and Morty classifier using Deep Learning Let us return to our example of image classification. We start by specifying a sequential network as before. model &lt;- keras_model_sequential() %&gt;% Our data is slightly different to the usual inputs we've been dealing with: that is, we're not dealing with an input vector, but instead have an array. In this case each image is a \\(90 \\times 160 \\time 3\\) array. So for our first layer we first have to flatten this down using {flatten}: model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(90,160,3)) This should turn our \\(90 \\times \\160 \\times 3\\) input into a \\(1 \\times 43200\\) node input. We now add intermediate layers connected to the input layer with rectified linear units ({relu}) as before. model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(90,160,3)) %&gt;% layer_dense(units = 100, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 120, activation = &quot;relu&quot;) Finally we connect this layer over the final output layer (one neuron) with sigmoid activation: activation layer_flatten(input_shape = c(90,160,3) , activation = &#39;relu&#39; ) %&gt;% layer_dense(units = 100) The complete model should look something like: model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(90,160,3)) %&gt;% layer_dense(units = 100, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 120, activation = &quot;relu&quot;) %&gt;% layer_dense(1, activation = &quot;sigmoid&quot;) We can print a summary of the network, for example to see how many parameters it has: summary(model) In this case we see a total of \\(4,332,341\\) parameters. Yikes, that's a lot of parameters to tune (well not compared some models), and not much data! Next we need to compile and run the model. In this case we need to specify the loss, optimiser, and metrics. Since we are dealing with binary classification, we will use binary cross entropy (binary_crossentropy) and for classification, a good choice of metrics would be {binary_accuracy}. We can compile our model using {keras_compile}: model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;binary_accuracy&quot;) Finally the model can be fitted to the data. When doing so we additionally need to specify the validation set (if we have one), the batch size and the number of epochs, where an epoch is one forward pass and one backward pass of all the training examples, and the batch size is the number of training examples in one forward/backward pass. You may want to go and get a tea whilst this is running! set.seed(12345) model %&gt;% fit(x = trainX, y = trainY, validation_data = list(valX, valY), epochs = 25, verbose = 2) Together with an added callback to save the best model, our code should look something like this: model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(90,160,3)) %&gt;% layer_dense(units = 100, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 120, activation = &quot;relu&quot;) %&gt;% layer_dense(1, activation = &quot;sigmoid&quot;) model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;binary_accuracy&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/model_rerun.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_binary_accuracy&quot;, verbose = 0) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainX, y = trainY, validation_data = list(valX, valY), epochs = 25, batch_size=500, verbose = 2, callbacks = list(cp_callback)) As before we can load a saved model in using the {load_model_hdf5} function and use it for predictions: model = load_model_hdf5(&#39;data/RickandMorty/data/models/model.h5&#39;) For this model we achieved an accuracy of above \\(0.62\\) on the validation dataset at epoch \\(20\\) (which had a corresponding accuracy \\(0.53\\) on the training set). Not fantastic when you consider that given the slight imbalance in the number of images in each class, a niave algorithm that always assigns the data to not Rick would achieve an accuracy of \\(0.58\\) and \\(0.57\\) in the training and validation sets respectively. It seems like we're getting nowhere fast, and need to change tactic. We need to think a little more about what the data actually is. In this case we're looking at a set of images. As Rick Sanchez can appear almost anywhere in the image, there's no reason to think that a given input node should correspond in two different images to any useful feature, so it's not surprising that the network did so badly, this is simply a task that a densely connected network is poor at. We need something that can extract out features from the image irregardless of where Rick is. There are approaches build precisely for image analysis that do just this: convolutional neural networks (CNN). 6.2 Convolutional neural networks Convolutional neural networks essentially scan across an image and extract out a set of feature representations. These features might then be passed on to a deeper layer (either a convolutional layer or other appropriate layer) which extract out higher order features, as shown in Figure 6.1. Finally, a densely connected network acts to combine the end features together for prediction, at least in an idealised description of what's going on. Figure 6.1: Example of a multilayer convolutional neural network In keras we can add a 2D convolutional layer using {layer_conv_2d}. A multilayer convolutional neural network might look something like: model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) which will construct \\(20\\) feature maps (using a kernel of size \\(5 \\times 5\\)). A \\(2 \\times 2\\) max-pool layer would then be added to condense down the feature representations: model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) In this case, by default the max-pooling slides one pixel at a time across the image, but we can set the max-pooling to take bigger steps by setting the {strides} option. A complete example model is shown below: model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_flatten( ) %&gt;% layer_dense(units=100) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units=1, activation = &quot;sigmoid&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelCNN_rerun.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_binary_accuracy&quot;, verbose = 0) model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;binary_accuracy&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainX, y = trainY, validation_data = list(valX, valY), epochs = 5, batch_size=100, verbose = 2, callbacks = list(cp_callback)) Here we only ran the model for \\(5\\) epochs just to get an feel for it. If we did have time to run this model longer, we would see much better accuracy. We have an accuracy of \\(0.88\\) on the validation dataset at epoch \\(43\\), with a training accuracy of \\(0.995\\). Whilst this is still not great (compared to how well a human could do on a similar task), it's accurate enough to begin making predictions and visualising the results. Fortunately I've already run (and saved) this model for \\(50\\) epochs, so let's load it in for predictions: model = load_model_hdf5(&#39;data/RickandMorty/data/models/modelCNN.h5&#39;) We can use this model to make predictions for images not present in either the training or validation datasets. We load in the new set of images, which can be found in the {predictions} subfolder: files &lt;- list.files(path = &quot;data/RickandMorty/data/predictions/&quot;,pattern = &quot;jpg&quot;) predictX &lt;- array(0,dim=c(length(files),90,160,3)) for (i in 1:length(files)){ x &lt;- readJPEG(paste(&quot;data/RickandMorty/data/predictions/&quot;, files[i],sep=&quot;&quot;)) predictX[i,1:90,1:160,1:3] &lt;- x[1:90,1:160,1:3] } A hard classification can be assigned using the {predict_classes} function, whilst the actual probability of assignment to either class can be evaluated using {predict} (this can be useful for images that might be ambiguous). probY &lt;- model %&gt;% predict(predictX) #Probability of it being class 1 i,e., a not Rick (1-this for prob of a Rick) predictY &lt;- as.numeric(probY&gt;0.5) #Hard assignment at p=0.5, a 1 if not Rick, and 0 if Rick We can plot an example: choice = 13 grid::grid.newpage() if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=25, col=&quot;black&quot;)) } choice = 1 grid::grid.newpage() if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=25, col=&quot;white&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } choice = 6 grid::grid.newpage() if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=25, col=&quot;black&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=25, col=&quot;white&quot;)) } grid::grid.newpage() choice = 16 if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick: Must be a Jerry&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;white&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.2, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;white&quot;)) } 6.2.1 Checking the models Although our model seems to be doing reasonably well, it always helps to see where things are going wrong. Let's take a look at a few of the false positives and a few of the false negatives. probvalY &lt;- model %&gt;% predict(valX) predictvalY &lt;-as.numeric(probvalY&gt;0.5) TP &lt;- which(predictvalY==1 &amp; valY==1) FN &lt;- which(predictvalY==0 &amp; valY==1) TN &lt;- which(predictvalY==0 &amp; valY==0) FP &lt;- which(predictvalY==1 &amp; valY==0) Let's see where we got it right. These were all predicted as not Rick, and the images did not contain a Rick: grid::grid.newpage() grid.raster(valX[TP[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[TP[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[TP[3],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) And these were all predicted as Rick and the image contained a Rick: grid::grid.newpage() grid.raster(valX[TN[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[TN[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[TN[3],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) And these we got wrong (predicted Rick, but he was't there): grid::grid.newpage() grid.raster(valX[FN[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[FN[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[FN[3],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) Or predicted not Rick, but he was in the image: grid::grid.newpage() grid.raster(valX[FP[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[FP[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[FP[4],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) Whilst visualising the data is often helpful, in this case it's not entirely clear why the network is failing in some of these cases. An alternative way to look at what's going wrong is a look at which pixels are contributing the most to the classifier, as we highligted in the lecture. We will return to this shortly. 6.3 Multiclass prediction In the previous section we dealt with predicting a Rick (0) or a not Rick (1). This is because we framed our goal as a simple binary question: an image either has a Rick in or it doesn't. Given the nature of the data, there are other possible questions we might be interested in: for example, does the image contain another character, such as Morty? A given image could both a Rick and a Morty, one or the other, or neither. We cold therefore frame our interst as two binary questions: &quot;does the image contain a Rick?&quot; and &quot;does the image contain a Morty?&quot; requiring two output nodes. I have already processed the invidual images into seperate folders, one folder containing only Rick, one containing only Morty, one containing both in the same image, and one containing neither. These can be read in: files1 &lt;- list.files(path = &quot;data/RickandMorty/data/ThreeClassModel/AllRickImages/&quot;, pattern = &quot;jpg&quot;) #These contain Rick but not Morty files2 &lt;- list.files(path = &quot;data/RickandMorty/data/ThreeClassModel/AllMortyImages/&quot;, pattern = &quot;jpg&quot;) #These contai Morty but not Rick files3 &lt;- list.files(path = &quot;data/RickandMorty/data/ThreeClassModel/Both/&quot;, pattern = &quot;jpg&quot;) #These contain both Rick and Morty files4 &lt;- list.files(path = &quot;data/RickandMorty/data/ThreeClassModel/Neither/&quot;, pattern = &quot;jpg&quot;) #These contain neither allX &lt;- array(0, dim=c(length(files1)+length(files2)+length(files3)+length(files4),dim(im)[1],dim(im)[2],dim(im)[3])) for (i in 1:length(files1)){ allX[i,1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllRickImages/&quot;, files1[i], sep=&quot;&quot;)) } for (i in 1:length(files2)){ allX[i+length(files1),1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllMortyImages/&quot;, files2[i], sep=&quot;&quot;)) } for (i in 1:length(files3)){ allX[i+length(files1)+length(files2),1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/Both/&quot;, files3[i], sep=&quot;&quot;)) } for (i in 1:length(files4)){ allX[i+length(files1)+length(files2)+length(files3),1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/Neither/&quot;, files4[i], sep=&quot;&quot;)) } As before we can costruct the output variable: a two dimensional output with \\([1,0]\\) indicating Rick, \\([0,1]\\) and Morty, \\([1,1]\\) indicating both, and \\([0,0]\\) neither. labels &lt;- rbind( t(t(rep(1, length(files1)))) %*% c(1,0), t(t(rep(1, length(files2)))) %*% c(0,1), t(t(rep(1, length(files3)))) %*% c(1,1), t(t(rep(1, length(files4)))) %*% c(0,0) ) In total there are \\(2213\\) Ricks, and \\(2158\\) Mortys. We can split this data for training/validation as before: set.seed(12345) #Set random number generator for R aspects of the session vecInd &lt;- seq(1,length(files1)+length(files2)+length(files3)+length(files4)) #A vector of indexes trainInd &lt;- sample(vecInd)[1:4001] #Permute and take first 4000 training #Train trainX &lt;- allX[trainInd, , , ] trainY &lt;- labels[trainInd, ] #Val valX &lt;- allX[-trainInd, , , ] valY &lt;- labels[-trainInd, ] And we can perform inference similarly to before, with the only real difference that we now have two output nodes with sigmoid activation: one asking is there a Rick, one asking is there a Morty. model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_flatten( ) %&gt;% layer_dense(units=100) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units=2, activation = &quot;sigmoid&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelCNNMultClass_rerun.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_binary_accuracy&quot;, verbose = 0) model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;binary_accuracy&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainX, y = trainY, validation_data = list(valX, valY), epochs = 5, batch_size=100, verbose = 2, callbacks = list(cp_callback)) 6.3.1 Categorical data In the previous example we were using a binary classification since we were only concerned if an the image contained a Rick or not (or by extension if a Morty or not). A more general case is categorical classification (of which binary is a special case) where we have \\(P\\) mutually exclusive classes, and wish to infer which one of those \\(P\\)-classes is in a particular image. In this case we might want to infer Rick, Morty, or neither, so a three class system. Note that since we are framing this as exclusive question, we shoudn't really use any data with both Rick and Morty in. allX &lt;- array(0, dim=c(length(files1)+length(files2)+length(files4),dim(im)[1],dim(im)[2],dim(im)[3])) for (i in 1:length(files1)){ allX[i,1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllRickImages/&quot;, files1[i], sep=&quot;&quot;)) } for (i in 1:length(files2)){ allX[i+length(files1),1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllMortyImages/&quot;, files2[i], sep=&quot;&quot;)) } for (i in 1:length(files4)){ allX[i+length(files1)+length(files2),1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/Neither/&quot;, files4[i], sep=&quot;&quot;)) } When using categorical data (Rick/Morty/Neither) we could instead represent this as a number or factor (1/2/3). This type of representation is still hard to use directly, and a much better represetation is via one-hot encoding, that is for each output we have vector of length \\(P\\) with zeros everywhere exept the column represeting a particular class. So in our Rick, Morty, Neither system a Rick is \\([1,0,0]\\), and Morty is \\([0,1,0]\\), and Neither is \\([0,0,1]\\). Given a list of observations from a \\(P\\)-class system we can one-hot encode using the {to_catogrical} function. In the snippet of code below we do this manually: labels &lt;- rbind( t(t(rep(1, length(files1)))) %*% c(1,0,0), t(t(rep(1, length(files2)))) %*% c(0,1,0), t(t(rep(1, length(files4)))) %*% c(0,0,1) ) In this particular case we have a slight class imbalace with \\(913\\) Ricks, \\(858\\) Mortys, and \\(2186\\) containing neither Rick nor Morty. As previously, we can construct a train/validation set: set.seed(12345) #Set random number generator for R aspects of the session vecInd &lt;- seq(1,length(files1)+length(files2)+length(files4)) #A vector of indexes trainInd &lt;- sample(vecInd)[1:3001] #Permute and take first 4000 training #Train trainX &lt;- allX[trainInd, , , ] trainY &lt;- labels[trainInd, ] #Val valX &lt;- allX[-trainInd, , , ] valY &lt;- labels[-trainInd, ] And finally we run the code. Here we must make sure to use the appropriate activation funtion on the final layer and correct loss function/metrics: model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_flatten( ) %&gt;% layer_dense(units=100) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units=3, activation = &quot;softmax&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelCNNCat_rerun.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_categorical_accuracy&quot;, verbose = 0) model %&gt;% compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;categorical_accuracy&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainX, y = trainY, validation_data = list(valX, valY), epochs = 5, batch_size=100, verbose = 2, callbacks = list(cp_callback)) Note that if we were specifically interested, we could set out to address the class weights using the {class_weights} argument in the model fitting step. In this case class \\(3\\) has almost \\(3\\) times more\\(1\\) and \\(2\\), so we want to upweight the underepresented classes by this amount: model %&gt;% fit(x = trainX, y = trainY, validation_data = list(valX, valY), class_weight = list(&quot;0&quot;=3,&quot;1&quot;=3,&quot;2&quot;=1), epochs = 5, batch_size=100, verbose = 2, callbacks = list(cp_callback)) I have already run this model for \\(50\\) epochs (without adjusting for class imbalance) and saved as a .h5 model. Let's take a quick look at this model for prediction: model = load_model_hdf5(&#39;data/RickandMorty/data/models/modelCNNCat.h5&#39;) probvalY &lt;- model %&gt;% predict(valX) #Find the instances where we the Rick probabilty is maximum and the image is a Rick: RickTP &lt;- which( (probvalY[,1]&gt;probvalY[,2]) &amp; (probvalY[,1]&gt;probvalY[,3]) &amp; valY[,1]==1 ) #Find the instances where we the Morty probabilty is maximum and the image is a Morty: MortyTP &lt;- which( (probvalY[,2]&gt;probvalY[,1]) &amp; (probvalY[,2]&gt;probvalY[,3]) &amp; valY[,2]==1 ) Let's plot them, for Rick: grid::grid.newpage() grid.raster(valX[RickTP[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[RickTP[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[RickTP[3],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) And for Morty: grid::grid.newpage() grid.raster(valX[MortyTP[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[MortyTP[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[MortyTP[3],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) 6.3.2 Intepreting CNN For these types of tasks, and besides the increaase performence, a key advantage of CNNs over densely connected networks lies in the increased interpretability. For example, for an optimised model and any given image we could take a look the feature representations at intermediate layers. In the snippet of code below, adapted from this tutorial (which itself builds on this work) we will take an example image and look at the activations in the first layer of the network: that is, we get a glimpse as to how the filters have processed a specific image. model = load_model_hdf5(&#39;data/RickandMorty/data/models/modelCNNCat.h5&#39;) tf$compat$v1$disable_eager_execution() layer_outputs &lt;- lapply(model$layers[1:8], function(layer) layer$output) activation_model &lt;- keras_model(inputs = model$input, outputs = layer_outputs) #Choice here is just an index for a given image. So here we just look at the 1st TP Rick image in the validation set choice = 1 activations &lt;- activation_model %&gt;% predict( array_reshape( valX[RickTP[choice],1:90,1:160,1:3] , c(1,90, 160, 3) ) ) first_layer_activation &lt;- activations[[1]] Recall that the first layer is an array of size \\(86 \\times 156 \\times 20\\). That is, 20 different feature representations of \\(86 \\times 146\\) (we can check the expected size of individual layers by looking at the model summary). Our output here should be the same as this. We will visualise some of these \\(20\\) feature representations (alongside the image itself). First the image: grid::grid.newpage() grid.raster( valX[RickTP[choice],1:90,1:160,1:3] , interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) And now the feature maps \\(1,2,3,4,5,10,12,15\\) and \\(20\\) (note the code is a little extra below, since the feature representations need to be rotated). op &lt;- par(mfrow=c(3,3)) image(t(first_layer_activation[1,,,1])[,ncol(t(first_layer_activation[1,,,1])):1], axes = FALSE ) image(t(first_layer_activation[1,,,2])[,ncol(t(first_layer_activation[1,,,2])):1], axes = FALSE ) image(t(first_layer_activation[1,,,3])[,ncol(t(first_layer_activation[1,,,3])):1], axes = FALSE ) image(t(first_layer_activation[1,,,4])[,ncol(t(first_layer_activation[1,,,4])):1], axes = FALSE ) image(t(first_layer_activation[1,,,5])[,ncol(t(first_layer_activation[1,,,5])):1], axes = FALSE ) image(t(first_layer_activation[1,,,10])[,ncol(t(first_layer_activation[1,,,10])):1], axes = FALSE ) image(t(first_layer_activation[1,,,12])[,ncol(t(first_layer_activation[1,,,12])):1], axes = FALSE ) image(t(first_layer_activation[1,,,15])[,ncol(t(first_layer_activation[1,,,15])):1], axes = FALSE ) image(t(first_layer_activation[1,,,20])[,ncol(t(first_layer_activation[1,,,20])):1], axes = FALSE ) par(op) We can see from these representations a general similarity to the original plot, but certain parts have been emphasized. We could similarly take a look at the feature representations deeper in the network: image_width &lt;- 56 images_per_row &lt;- 5 i &lt;- 7 layer_activation &lt;- activations[[i]] layer_name &lt;- model$layers[[i]]$name n_features &lt;- dim(layer_activation)[[4]] n_cols &lt;- n_features %/% images_per_row op &lt;- par(mfrow = c(n_cols, images_per_row), mai = rep_len(0.02, 4)) for (col in 0:(n_cols - 1)) { for (row in 0:(images_per_row - 1)) { channel_image &lt;- layer_activation[1,,,(col*images_per_row) + row + 1] image(t(channel_image)[,ncol(t(channel_image)):1], axes = FALSE,) } } par(op) At higher layers, the features have become far less recognisable and now, and represent specific features that can be built up to perform classification. 6.3.2.1 Class activation Further to visualising the feature represetations, we can also begin to visualise how particular regions activate to a particular class i.e., identify what pixels the CNN is concentrating on when looking classifying an image as Rick. choice &lt;- 7 X0 &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllRickImages/&quot;, files1[choice], sep=&quot;&quot;)) imag_pred &lt;- array_reshape(X0[1:90,1:160,1:3] , c(1, 90, 160, 3) ) predict(model, imag_pred) # Prediction vector, in this case class 1 is Rick d_output &lt;- model$output[, 1] #The last convolutional layer (you will need to know what this layer is called for your network, which can be done by looking at the model summary. In this case, because you&#39;re using my precalculated network there is no need to change things) last_conv_layer &lt;- model %&gt;% get_layer(&quot;conv2d_20&quot;) # This is the gradient of the &quot;Rick&quot; class with respect to the output feature map of `conv2d_20` grads &lt;- k_gradients(d_output, last_conv_layer$output)[[1]] pooled_grads &lt;- k_mean(grads, axis = c(1, 2, 3)) #Function to access the values of `pooled_grads` and output feature map of `conv2d_20`, given a sample image iterate &lt;- k_function(list(model$input),list(pooled_grads, last_conv_layer$output[1,,,])) c(pooled_grads_value, conv_layer_output_value) %&lt;-% iterate(list( imag_pred )) #Multiply each channel in the feature map array by &quot;how important this channel is&quot; with regard to the Rick class for (i in 1:64) { conv_layer_output_value[,,i] &lt;- conv_layer_output_value[,,i] * pooled_grads_value[[i]] } #Channel-wise mean of the resulting feature map is our heatmap of class activation heatmap &lt;- apply(conv_layer_output_value, c(1,2), mean) #A whole lot of plotting: heatmap &lt;- pmax(heatmap, 0) heatmap &lt;- heatmap / max(heatmap) write_heatmap &lt;- function(heatmap, filename, width = 150, height = 150,bg = &quot;white&quot;, col = terrain.colors(12)) { png(filename, width = width, height = height, bg = bg) op = par(mar = c(0,0,0,0)) on.exit({par(op); dev.off()}, add = TRUE) rotate &lt;- function(x) t(apply(x, 2, rev)) image(rotate(heatmap), axes = FALSE, asp = 1, col = col) } write_heatmap(heatmap, &quot;data/RickandMorty/RM_heatmap.png&quot;) library(magick) library(viridis) image &lt;- image_read(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllRickImages/&quot;, files1[choice], sep=&quot;&quot;)) info &lt;- image_info(image) geometry &lt;- sprintf(&quot;%dx%d!&quot;, info$width, info$height) pal &lt;- col2rgb(viridis(20), alpha = TRUE) alpha &lt;- floor(seq(0, 255, length = ncol(pal))) pal_col &lt;- rgb(t(pal), alpha = alpha, maxColorValue = 255) write_heatmap(heatmap, &quot;data/RickandMorty/RM_overlay.png&quot;, width = 14, height = 14, bg = NA, col = pal_col) # Overlay the heatmap image_read(&quot;data/RickandMorty/RM_overlay.png&quot;) %&gt;% image_resize(geometry, filter = &quot;quadratic&quot;) %&gt;% image_composite(image, operator = &quot;blend&quot;, compose_args = &quot;20&quot;) %&gt;% plot() We can have a look at where the CNN focuses on a set of Morty images. choice &lt;- 30 X0 &lt;- readJPEG(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllMortyImages/&quot;, files2[choice], sep=&quot;&quot;)) imag_pred &lt;- array_reshape(X0[1:90,1:160,1:3] , c(1, 90, 160, 3) ) predict(model, imag_pred) d_output &lt;- model$output[, 2] last_conv_layer &lt;- model %&gt;% get_layer(&quot;conv2d_20&quot;) grads &lt;- k_gradients(d_output, last_conv_layer$output)[[1]] pooled_grads &lt;- k_mean(grads, axis = c(1, 2, 3)) iterate &lt;- k_function(list(model$input),list(pooled_grads, last_conv_layer$output[1,,,])) c(pooled_grads_value, conv_layer_output_value) %&lt;-% iterate(list( imag_pred )) for (i in 1:64) { conv_layer_output_value[,,i] &lt;- conv_layer_output_value[,,i] * pooled_grads_value[[i]] } heatmap &lt;- apply(conv_layer_output_value, c(1,2), mean) heatmap &lt;- pmax(heatmap, 0) heatmap &lt;- heatmap / max(heatmap) write_heatmap &lt;- function(heatmap, filename, width = 150, height = 150, bg = &quot;white&quot;, col = terrain.colors(12)) { png(filename, width = width, height = height, bg = bg) op = par(mar = c(0,0,0,0)) on.exit({par(op); dev.off()}, add = TRUE) rotate &lt;- function(x) t(apply(x, 2, rev)) image(rotate(heatmap), axes = FALSE, asp = 1, col = col) } write_heatmap(heatmap, &quot;data/RickandMorty/RM_heatmap.png&quot;) library(magick) library(viridis) image &lt;- image_read(paste(&quot;data/RickandMorty/data/ThreeClassModel/AllMortyImages/&quot;, files2[choice], sep=&quot;&quot;)) info &lt;- image_info(image) geometry &lt;- sprintf(&quot;%dx%d!&quot;, info$width, info$height) pal &lt;- col2rgb(viridis(20), alpha = TRUE) alpha &lt;- floor(seq(0, 255, length = ncol(pal))) pal_col &lt;- rgb(t(pal), alpha = alpha, maxColorValue = 255) write_heatmap(heatmap, &quot;data/RickandMorty/RM_overlay.png&quot;, width = 14, height = 14, bg = NA, col = pal_col) image_read(&quot;data/RickandMorty/RM_overlay.png&quot;) %&gt;% image_resize(geometry, filter = &quot;quadratic&quot;) %&gt;% image_composite(image, operator = &quot;blend&quot;, compose_args = &quot;20&quot;) %&gt;% plot() 6.4 CNNs for Motif analysis Aside from image analyses, CNNs have also bee useful for studying other types of data including genomic data such as DNA-sequence analysis, and DNA-methylation or histone modificaiton data. The use of CNNs for studying regulatory motifs in genomic sequecing data as been reviewed in (Zhang et al. 2022). Alhough no longer considered state-of-the art, CNNs for these applications still find practical use, particularly when combined with more recent developments such as recurrent neural networks. In the example below we code for a simple CNN which aims to identify if a particular \\(200\\)bp genomic region contains a SOX17 or PRDM1 binding site. The data is based on ChIP-sequencig data taken from (Tang et al. 2022) and is avaible to download at NCBI GEO (GSE159654). Briefly, this dataset consists of ChIP-sequencing indicating the binding of two trascription factors, SOX17 and PRDM1, that play a role in the specification of the germline. Here the work makes use of an in vitro model based on pluripotent stem cells to derived embryonnic precursors of sperm and eggs, the primordial germ cells. For this section, peaks from individual replicates were concatenated and overlapping regions merged using bedtools, and a final list of regions for each TF was generated based on the centre of these peaks plus or minus \\(100\\)bp. Regions with both a SOX17 and PRDM1 binding site were excluded. For further comparison a random set of genomic sequences were sampled from the human genome. Finallly, geomic sequencecs associated with human genome hg38 extracted using bedtools getfasta. The processing steps can be found in the file data/ChIP/processdata.sh. In the sippet of code below we use the Biostrings package to read in the fasta files for SOX17, PRDM1, and random regions. Dealing directly with a sequence string is difficult, so to make inferece easier we first one-hot the sequence s i.e., instead of representing a particular base pair as A, C, G, or T, these will instead be represened as \\(A \\to [1,0,0,0]\\), \\(C \\to [0,1,0,0]\\), \\(G \\to [0,0,1,0]\\), \\(T \\to [0,0,0,1]\\). Each basepair will be treated as a seperate &quot;colour channel&quot;, and the iput data is a \\(200 \\timmes 4\\) array. In total we have \\(13,065\\) sequences for SOX17, \\(22,255\\) for PRDM1, and \\(21,921\\) random regions. library(Biostrings) ## Warning: package &#39;Biostrings&#39; was built under R version 3.5.2 ## Loading required package: BiocGenerics ## Loading required package: parallel ## ## Attaching package: &#39;BiocGenerics&#39; ## The following objects are masked from &#39;package:parallel&#39;: ## ## clusterApply, clusterApplyLB, clusterCall, clusterEvalQ, ## clusterExport, clusterMap, parApply, parCapply, parLapply, ## parLapplyLB, parRapply, parSapply, parSapplyLB ## The following object is masked from &#39;package:keras&#39;: ## ## normalize ## The following objects are masked from &#39;package:dplyr&#39;: ## ## combine, intersect, setdiff, union ## The following objects are masked from &#39;package:stats&#39;: ## ## IQR, mad, sd, var, xtabs ## The following objects are masked from &#39;package:base&#39;: ## ## anyDuplicated, append, as.data.frame, basename, cbind, colMeans, ## colnames, colSums, dirname, do.call, duplicated, eval, evalq, ## Filter, Find, get, grep, grepl, intersect, is.unsorted, lapply, ## lengths, Map, mapply, match, mget, order, paste, pmax, pmax.int, ## pmin, pmin.int, Position, rank, rbind, Reduce, rowMeans, rownames, ## rowSums, sapply, setdiff, sort, table, tapply, union, unique, ## unsplit, which, which.max, which.min ## Loading required package: S4Vectors ## Loading required package: stats4 ## ## Attaching package: &#39;S4Vectors&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## first, rename ## The following object is masked from &#39;package:base&#39;: ## ## expand.grid ## Loading required package: IRanges ## ## Attaching package: &#39;IRanges&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## collapse, desc, slice ## Loading required package: XVector ## ## Attaching package: &#39;Biostrings&#39; ## The following object is masked from &#39;package:base&#39;: ## ## strsplit library(abind) SOX17 &lt;- readDNAStringSet(&quot;data/ChIP/SOX17.fa&quot;) SOXseq &lt;- paste(SOX17) allS17 &lt;- array(0, dim=c(length(SOXseq),200,4)) for (i in 1:length(SOXseq)){ allS17[i,1:200,1:4] &lt;- diag(4)[match(unlist(lapply(SOXseq[i], utf8ToInt)), utf8ToInt(&quot;ACGT&quot;)), ] } PRDM1 &lt;- readDNAStringSet(&quot;data/ChIP/PRDM1.fa&quot;) PRDMseq &lt;- paste(PRDM1) allP1 &lt;- array(0, dim=c(length(PRDMseq),200,4)) for (i in 1:length(PRDMseq)){ allP1[i,1:200,1:4] &lt;- diag(4)[match(unlist(lapply(PRDMseq[i], utf8ToInt)), utf8ToInt(&quot;ACGT&quot;)), ] } RANDOM1 &lt;- readDNAStringSet(&quot;data/ChIP/random.fa&quot;) Rseq &lt;- paste(RANDOM1) allR1 &lt;- array(0, dim=c(length(Rseq),200,4)) for (i in 1:length(Rseq)){ allR1[i,1:200,1:4] &lt;- diag(4)[match(unlist(lapply(Rseq[i], utf8ToInt)), utf8ToInt(&quot;ACGT&quot;)), ] } #We need to filter out NNNN sequeces allR1 &lt;- allR1[is.na(rowSums(rowSums(allR1, dims = 2), dims = 1 ))==FALSE, , ] allTFX &lt;- abind(allS17, allP1, allR1, along = 1) Based on the way we have processed the data, our ChIP dataset represets a \\(3\\) class system: any given sequence may contain either a SOX17-binding region, a PRDM1-binding region, or a randomly sampled sequece without SOX17/PRDM1-binding. For this example we could therefore treat this as categorical classification, with the peaks one-hot encoded, such that a SOX17 peak is represented as \\([1,0,0]\\), a PRDM1 peak as \\([0,1,0]\\), and the random sequences as \\([0,0,1]\\). Below we generate a random training and test set for the model by radomly splitting the dataset (\\(40,000\\) for training and \\(17,241\\) for validation). In general, this may not be the optimal way to split the data, particularly if we are interested in esuring the inferences are generalisable. Other ways to split would be to train on data from a subset of chromosomes (or on one replicate) and test on the remainder (or independent biological replicate); alteratively, in some casaes, it may be of interest to train on data from one cell type or condition type and predict on another, or even train on one species and predict an another. There are caveats, of course, and such splitting won't always be appropriate; ultimately, the decision will be influenced by our biological questions. labels &lt;- rbind( t(t(rep(1, dim(allS17)[1] ))) %*% c(1,0,0), t(t(rep(1, dim(allP1)[1] ))) %*% c(0,1,0), t(t(rep(1, dim(allR1)[1] ))) %*% c(0,0,1) ) vecInd &lt;- seq(1, dim(allTFX)[1] ) #A vector of indexes trainInd &lt;- sample(vecInd)[1:40000] #Permute and take first 4000 training allTFX_train &lt;- allTFX[trainInd, , ] allTFX_test &lt;- allTFX[-trainInd, ,] allYtrain &lt;- labels[trainInd,] allYtest &lt;- labels[-trainInd,] Finally, we are ready to encode a model and perform inference. Here we have a three class classification system with a sequence associated with either SOX17, PRDM1, or a random region, so can use a categorical approach (thus use a softmax activation on the final level and use categorical cross etrophy and accuracy). Technically, we only have mutually exclusive categories because we have filtered the regios to esure there are no overlapping ones; within the genome we may - and in our case do - get cases where SOX17/PRDM1 co-bind, so if we were interested in this aspect of biology, we might istead treat our data as a two-node binary classification for SOX17 and PRDM1. Below we code a simple CNN consisting of two 1D convolution layers (each with a 1D pooling layer). model &lt;- keras_model_sequential() %&gt;% layer_conv_1d(input_shape = list(200,4), filters = 20, kernel_size = c(5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_1d(pool_size=c(2)) %&gt;% layer_conv_1d(filters = 64, kernel_size = c(5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_1d(pool_size=c(2)) %&gt;% layer_flatten( ) %&gt;% layer_dense(units=100) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units=3, activation = &quot;softmax&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelCNNTF_rerun.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_categorical_accuracy&quot;, verbose = 0) model %&gt;% compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;sgd&quot;, metrics = &quot;categorical_accuracy&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = allTFX_train, y = allYtrain , validation_data = list(allTFX_test, allYtest), epochs = 5, batch_size=1000, verbose = 0, callbacks = list(cp_callback)) If we run this snippet of code we see the beginings of a increase in accuracy; this accuracy begins to plateau after around \\(300\\) epochs (I have saved this model as 'data/RickandMorty/data/models/modelCNNTF.h5') where it hits around \\(70\\%\\) accuracy. We have thus trained an algorithm to predict if a particcualr genomic sequence is a potential target of a particular TF. We could, of course, aim to increase this accuracy by tweaking the arcitecture, and the here we might want to follow the mantra &quot;don't be a hero&quot;. That is, try arcitectures that have been shown to work well on these sorts of tasks. I will leave this to the individuals to play around with. For ispiration see the various approaches in (Zhang et al. 2022). Excercise 2.2: Try visualising what the algorithm is looking at within a sequence region (hint: this is slightly different to image analysis. Start with a given motif and calculating how much the probability of mapping to the correct label changes when you perturb one base pair e.g., set that basepair to \\([0,0,0,0]\\), then roll this out systematically for each base pair in turn). A useuful source for inspiration can be found here 6.4.1 Data augmentation Although we saw some improvements when using convolutional neural networks compared to densely connected one, the end results were not particularly convincing. After all, previous applications in the recognition of handwritten digits (0-9) showed above human accuracy, see e.g., Neural Networks and Deep Learning. Our accuracy for image analysis pushed approximately \\(90\\) percent, whilst our TF example was closer to \\(70\\) percent, neither of which close to human levels. So where are we gong wrong? We should, of course, start by considering the number of parameters versus the size of the training dataset. In our final model we had many parameters, and only a few thousand training images, so it is perhaps not surprising that our model is doing relatively poorly. In previous examples of digit recognition more than \\(10,000\\) images were used, whilst better known examples of deep learning for image classification make use of millions of images. Our task is also, arguably, a lot harder than digit recognition. After all, a handwritten \\(0\\) is relatively similar regardless of who wrote it. Rick Sanchez, on the other hand, can come in a diverse range of guises, with different postures, facial expressions, clothing, and even in pickle-Rick form. We may well need a vastly increased number of training images: with more training data, we can begin to learn more robustly what features define a Rick. Whilst we could simply download more data from Master of All Science, an alternative approach is to artificially increase our pool of training data by manipulating the images. For example, we could shear, warp or rotate some of the images in our training set; we could add noise and we could manipulate the colouring. This can be done using the data generators ({image_data_generator}) alongside ({flow_images_from_directory}), which will essentially load data in from a folder, modify it in some customisable sort of way, and pass over to the network for training, essentially allowing infinite data from a limited one. Whilst this is certainly useful, and often increases accuracy, one should be careful that the new &quot;infinite&quot; data source is really representative and that you don't end up simply learning the specifics of a subset of your training/validation set. Careful consideration also needs to be give as to what alterationns to the dataset means: for example, shearing or slightly colour changes to a Rick still leaves a Rick, however random perturbations to DNA-sequence data could, in reality, change the class, particularly if you hit the exat binding region. An alternative for sequence data would be a shift, i.e, move the sequence a random distance up or downstream (so long as you think the core of whatever you aim to capture -- usually a small motif -- remains with the sequence itself). Excercise 2.3: Try coding a CNN for image analysis using laading data direct from their folders, rather than loading each to memory. Hint: look up the functions {image_data_generator} and {flow_images_from_directory}. 6.4.2 Transfer learning Another approach that might help us increase our accuracy is to use transfer learning. This is where we make use of existing neural networks to make predictions about our specific datasets, usually by fixing the topology and parameters of the uppermost layers and fine tuning the lower layers to our dataset. For image recognition we could make use of top perfoming neural networks on the ImageNet database, although these types of large-scale models are certainly not without their issues (Prabhu and Birhane 2020). Whilst none of these networks would have been designed to identify Rick they would have been trained on millions of images, and the top level representations would have been able to extract useful general features that allowed identification of images which might hold for our images too. 6.4.3 More complex networks More complex learning algorithms can easily be built using Keras via the model class API. This allows, for example, learning from multiple inputs and/or predicting multiple outputs, with more interconnection between the different layers. We might, for example, want to include additional contextual information about the image that could serve to augment the predictions. For our sequence data we might want to allow several different filters of different sizes to run across the sequence, effectively allowing motifs of different sizes to be present in the dataset. 6.4.4 Autoencoders In previous sections we have used CNNs to build a Rick/not Rick classifier. In doing so we are halfway towards other interesting neural network architectures, including autoencoders. One type of autoencoder consists of a stack of convolution/max pooling layers which served to condense the original image down into a reduced dimensional (encoded) representation, with a stack of upsampled layers used to decode the encoded layer (Figure 6.2). Within such a network the input and output data are an identical image: we are therefore training a network that can both compresses the original high resolution data and subsequently interpret that compressed representation to recreate the original as closely as possible. A slight deviation of this principle would be to use noisy versions of the image as input, with clean versions as the output. In these cases the autoencoder becomes a denoiser (Figure 6.3). Similar methods can be used for generating higher resolution versions of an image (or from noise). Figure 6.2: Example of an autoencoder Figure 6.3: Example of an autoencoder In the example below we implement a simple Autoencoder, constructed by stacking a number of convolution layers with a stak of deconvolution layers (foregoing the max pooling layers). Note that in, in R, each pixel is represented as a number between 1 and 0. A suitable final activation function is therefore one that scales between 0 and 1 e.g., a sigmoid function. Nevertheless, we are not doing logistic regrssion, so we will choose to monitor the mse. Note that this snippet of code will take a good few hours to run \\(50\\) epochs. model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d_transpose(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d_transpose(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d_transpose(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d(filters = 3, kernel_size = c(5,5), padding = &#39;same&#39;) %&gt;% layer_activation(&quot;sigmoid&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelAE_rerun.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_mse&quot;, verbose = 0) model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;mse&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = trainX, y = trainX, validation_data = list(valX, valX), epochs = 25, verbose = 2, callbacks = list(cp_callback)) Instead of running this snippet again, we can load in a pre-run model I already saved. model = load_model_hdf5(&#39;data/RickandMorty/data/models/modelAE.h5&#39;) summary(model) ## Model: &quot;sequential_7&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## conv2d_3 (Conv2D) (None, 86, 156, 20) 1520 ## ## activation_3 (Activation) (None, 86, 156, 20) 0 ## ## conv2d_4 (Conv2D) (None, 82, 152, 20) 10020 ## ## activation_4 (Activation) (None, 82, 152, 20) 0 ## ## conv2d_5 (Conv2D) (None, 78, 148, 64) 32064 ## ## activation_5 (Activation) (None, 78, 148, 64) 0 ## ## conv2d_transpose (Conv2DTranspose) (None, 82, 152, 64) 102464 ## ## activation_6 (Activation) (None, 82, 152, 64) 0 ## ## conv2d_transpose_1 (Conv2DTranspos (None, 86, 156, 20) 32020 ## e) ## ## activation_7 (Activation) (None, 86, 156, 20) 0 ## ## conv2d_transpose_2 (Conv2DTranspos (None, 90, 160, 20) 10020 ## e) ## ## activation_8 (Activation) (None, 90, 160, 20) 0 ## ## conv2d_6 (Conv2D) (None, 90, 160, 3) 1503 ## ## activation_9 (Activation) (None, 90, 160, 3) 0 ## ## ================================================================================ ## Total params: 189,611 ## Trainable params: 189,611 ## Non-trainable params: 0 ## ________________________________________________________________________________ We can see that this model condenses down the images from \\(90 \\times 160\\) pixel images down to \\(78 \\times 148\\) (not a huge compression, but a good starting point to illustrate thigs). Let's try compressing (and decompressing) a few of the held out examples: predictAEX &lt;- model %&gt;% predict(predictX) grid::grid.newpage() grid.raster(predictX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(predictAEX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid::grid.newpage() grid.raster(predictX[2,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(predictAEX[2,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid::grid.newpage() grid.raster(predictX[3,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(predictAEX[3,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) Exercise 2.4: Think about how the script can be modified to demonstrate the use of a denoisiny algorithm (hint: the dataset will need to be modified in some way, but the algorithm itself should be functional as is). 6.5 Further reading A particularly comprehensive introduction to Deep Learning can be found in Neural Networks and Deep Learning, written by Michael Nielsen. Useful examples can also be found in the keras documentation, with many more examples found in the keras R wrapper documentation. Deep Learning with R is also a fatastic resource for those wishing to continue in this field. ======= ## Exercises Solutions to exercises can be found in appendix 8. References "],
["solutions-logistic-regression.html", "7 Solutions to Chapter 4 - Linear regression and logistic regression", " 7 Solutions to Chapter 4 - Linear regression and logistic regression Solutions to exercises of chapter 5. We can systematically fit a model with increasing degree and evaluate/plot the RMSE on the held out data. RMSE &lt;- rep(NULL, 10) lrfit1 &lt;- train(y~poly(x,degree=1), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[1] &lt;- lrfit1$results$RMSE lrfit2 &lt;- train(y~poly(x,degree=2), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[2] &lt;- lrfit2$results$RMSE lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[3] &lt;- lrfit3$results$RMSE lrfit4 &lt;- train(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[4] &lt;- lrfit4$results$RMSE lrfit5 &lt;- train(y~poly(x,degree=5), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[5] &lt;- lrfit5$results$RMSE lrfit6 &lt;- train(y~poly(x,degree=6), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[6] &lt;- lrfit6$results$RMSE lrfit7 &lt;- train(y~poly(x,degree=7), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[7] &lt;- lrfit7$results$RMSE lrfit8 &lt;- train(y~poly(x,degree=8), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[8] &lt;- lrfit8$results$RMSE lrfit9 &lt;- train(y~poly(x,degree=9), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[9] &lt;- lrfit9$results$RMSE lrfit10 &lt;- train(y~poly(x,degree=10), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[10] &lt;- lrfit10$results$RMSE plot(RMSE) plot(RMSE[1:5]) From these plots it looks like the best model is one with degree \\(d=2\\) or \\(d=4\\), suggesting there is a lot more complexity to this gene. You can clean the code up to make it run in a loop. Hint: you can not directly pass a variable over to poly (y~poly(x,i) will not work) and will have to convert to a function: setdegree &lt;- 5 f &lt;- bquote( y~poly(x,degree=.(setdegree) ) ) lrfit11 &lt;- train( as.formula(f) , data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) Excercise 1.1: Here we simply specify two three models: the first a regression run on the union of the data represents the case of no DE; the second is two independent models, one for each time series. In cases where there is no DE, two independenet models will not be necessary to describe the data compared to independent ones, and the mean square error will be similar. See also (Stegle et al. 2010). Excercise 1.2: We have time series, we are interested in inferring the regultors of a particular gene. We can therefore regress the time seris of the gene of interest (at time point \\(2\\) through \\(T\\)) against combinations of putative regulators at the previous time point (at time point \\(1\\) through \\(T-1\\)), and use the an appropriate metric to select the optimal combinations. We can do this in parallel for all genes to arrive at a network. For further details see e.g., (Penfold and Wild 2011) and (Penfold et al. 2019) References "],
["solutions-nnet.html", "8 Solutions to Chapter 5 - Neural Networks", " 8 Solutions to Chapter 5 - Neural Networks Excersie 2.1: We can simply update the input dimension and output dimensions: model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(5)) %&gt;% layer_dense(units = 10, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20, activation = &quot;relu&quot;) %&gt;% layer_dense(3, activation = &quot;linear&quot;) Excercsie 2.2: The network architecture should be fine for this task. However a noisy version of the input data will have to be generated (e.g., by setting a random set of pixels to zero) to be passed in to the AE. A clean version of the data should be retained and passed to the AE as the output. model = load_model_hdf5(&#39;data/RickandMorty/data/models/modelCNNTF.h5&#39;) tf$compat$v1$disable_eager_execution() layer_outputs &lt;- lapply(model$layers[1:8], function(layer) layer$output) activation_model &lt;- keras_model(inputs = model$input, outputs = layer_outputs) choice = 1 activations &lt;- activation_model %&gt;% predict( array_reshape( allTFX_test[choice, ,] , c(1,200, 4) ) ) first_layer_activation &lt;- activations[[1]] op &lt;- par(mfrow=c(1,1)) image((first_layer_activation[1,,]), axes = FALSE ) par(op) For a given sequence we can get an idea of the importance of any given basepair by artifically setting that basepair to \\([0,0,0,0]\\) and see how that effects the probabilty of mapping to the correct class. Let's pick a particular sequence and take a look: model = load_model_hdf5(&#39;data/RickandMorty/data/models/modelCNNTF.h5&#39;) seqchoice &lt;- 1 pr1 &lt;- model %&gt;% predict(array_reshape(allTFX_test[seqchoice, ,],c(1,200,4) )) Delta &lt;- array(0, dim=c(200,200,4)) for (i in 1:200){ Delta[i,,] &lt;- allTFX_test[seqchoice, ,] Delta[i,i,1:4] &lt;- 0 } pr &lt;- model %&gt;% predict( Delta ) DeltaP &lt;- pr1[1,1] - pr[,1] ggplot(data.frame(x=seq(1,200,1),y=DeltaP ), aes(x = x, y = y)) + geom_line(size = 1) + geom_point(color=&#39;blue&#39;) + theme_bw() so we can see a peak region somewhere betwee 125 and 150 ggplot(data.frame(x=seq(128,138,1),y=DeltaP[128:138] ), aes(x = x, y = y)) + geom_line(size = 1) + geom_point(color=&#39;blue&#39;) + theme_bw() Let's take a look at the sequence here: #BiocManager::install(&quot;motifStack&quot;) #BiocManager::install(&quot;universalmotif&quot;) library(motifStack) library(universalmotif) motif &lt;- allTFX_test[seqchoice,128:138 ,] colnames(motif) &lt;- c(&quot;A&quot;,&quot;C&quot;,&quot;G&quot;,&quot;T&quot;) motif reversemotif &lt;- motif reversemotif[,c(&quot;A&quot;)] &lt;- motif[,c(&quot;T&quot;)] reversemotif[,c(&quot;T&quot;)] &lt;- motif[,c(&quot;A&quot;)] reversemotif[,c(&quot;C&quot;)] &lt;- motif[,c(&quot;G&quot;)] reversemotif[,c(&quot;G&quot;)] &lt;- motif[,c(&quot;C&quot;)] motif&lt;-new(&quot;pfm&quot;, mat=as.matrix(t(motif) ), name=&quot;CAP&quot;, color=colorset(alphabet=&quot;DNA&quot;,colorScheme=&quot;basepairing&quot;)) reversemotif&lt;-new(&quot;pfm&quot;, mat=as.matrix(t(reversemotif) ), name=&quot;CAP&quot;, color=colorset(alphabet=&quot;DNA&quot;,colorScheme=&quot;basepairing&quot;)) Sox17pwm &lt;- t(matrix( c(7,8,3,30,0,0,0,0,0, 9,8,18,0,1,0,0,0, 17,6,4,1,0,0,0,31,2,10, 9,11,9,1,30,31,0,29,4), nrow=4, ncol=9, byrow = TRUE)) colnames(Sox17pwm) &lt;- c(&quot;A&quot;,&quot;C&quot;,&quot;G&quot;,&quot;T&quot;) Sox17pwm&lt;-new(&quot;pfm&quot;, mat=as.matrix(t(Sox17pwm) ), name=&quot;CAP&quot;, color=colorset(alphabet=&quot;DNA&quot;,colorScheme=&quot;basepairing&quot;)) op &lt;- par(mfrow=c(1,3)) view_motifs(Sox17pwm, use.type = &quot;PPM&quot;) view_motifs(motif, use.type = &quot;PPM&quot;) view_motifs(reversemotif, use.type = &quot;PPM&quot;) par(op) Although these approaches are no longer considered state of the art, they still have some practical value, and have been incorporated into more complex arcitectures which, e.g., combined CNN to learn motifs with LSTM to learn long range interactions. model &lt;- keras_model_sequential() forward_layer = layer_lstm(units = 1024, return_sequences=TRUE) backward_layer = layer_lstm(units = 1024, activation=&#39;relu&#39;, return_sequences=TRUE,go_backwards=TRUE) model %&gt;% layer_conv_1d(input_shape = list(200,4), filters = 1024, kernel_size = c(30)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_1d(pool_size=c(15),strides = c(4)) %&gt;% layer_dropout(rate = 0.2) %&gt;% bidirectional(layer = forward_layer,backward_layer=backward_layer) %&gt;% layer_flatten( ) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_flatten( ) %&gt;% layer_dense(units=100) %&gt;% layer_dense(units = 3, activation = &quot;sigmoid&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelCNNRNN.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_categorical_accuracy&quot;, verbose = 0) model %&gt;% compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;adadelta&quot;, metrics = &quot;categorical_accuracy&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = allTFX_train, y = allYtrain , validation_data = list(allTFX_test, allYtest), epochs = 300, batch_size=1000, verbose = 2, callbacks = list(cp_callback)) Excercise 2.3: read images direct from their folder. #Number of files in total around 5257, we will split roughly by number_of_train_samples &lt;- 4000 number_of_val_samples &lt;- 1257 batch_size = 100 steps_per_epoch = ceiling(number_of_train_samples / batch_size) val_steps = ceiling(number_of_val_samples / batch_size) datagen &lt;- image_data_generator(horizontal_flip = TRUE, validation_split = 0.2) test_generator = flow_images_from_directory(&quot;data/RickandMorty/altdata/&quot;, target_size=c(90, 160), batch_size = batch_size, class_mode=&#39;binary&#39;,shuffle=FALSE, seed=10, subset = &#39;validation&#39;, color_mode = &#39;rgb&#39;,generator = datagen) train_generator = flow_images_from_directory(&quot;data/RickandMorty/altdata/&quot;, target_size=c(90, 160), batch_size = batch_size, class_mode=&#39;binary&#39;,shuffle=FALSE, seed=10, subset = &#39;training&#39;, color_mode = &#39;rgb&#39;,generator = datagen) model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% layer_flatten( ) %&gt;% layer_dense(units=100) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units=1, activation = &quot;sigmoid&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelCNNFlowfromfolder.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_binary_accuracy&quot;, verbose = 0) model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;binary_accuracy&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(train_generator, steps_per_epoch= steps_per_epoch, validation_data = test_generator, validation_steps= val_steps, epochs = 5, verbose = 2, callbacks = list(cp_callback)) Excercise 2.4: The same sippet of code should be usable from the image analyses, with minor changes to &quot;image size&quot;. We first randomly set a certain fraction of pixels to 0. cleanX &lt;- valX noiseX &lt;- valX fraction_of_pixels &lt;- 0.25 for (i in 1:dim(noiseX)[1]) { Npix = prod(dim(noiseX)[2:3]) Rpix = sample(Npix, fraction_of_pixels * Npix) R &lt;- noiseX[i,,,1] G &lt;- noiseX[i,,,2] B &lt;- noiseX[i,,,3] R[Rpix] &lt;- 0 G[Rpix] &lt;- 0 B[Rpix] &lt;- 0 noiseX[i,,,1] = R noiseX[i,,,2] = G noiseX[i,,,3] = B } grid::grid.newpage() grid.raster(noiseX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(cleanX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) We can the train on the autoencoder to denoise the image. model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(input_shape = list(90,160,3), filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d_transpose(filters = 64, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d_transpose(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d_transpose(filters = 20, kernel_size = c(5,5)) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_conv_2d(filters = 3, kernel_size = c(5,5), padding = &#39;same&#39;) %&gt;% layer_activation(&quot;sigmoid&quot;) cp_callback &lt;- callback_model_checkpoint(filepath = &#39;data/RickandMorty/data/models/modelAEND_rerun.h5&#39;,save_weights_only = FALSE, mode = &quot;auto&quot;, monitor = &quot;val_mse&quot;, verbose = 0) model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;mse&quot;) tensorflow::set_random_seed(42) model %&gt;% fit(x = noiseX, y = cleanX, validation_split=0.25, epochs = 5, verbose = 2, callbacks = list(cp_callback)) Fortunately I've already run this model for 50 epochs. We can therefore load it in and use to visualise the de-noising. The bottom image is the clean data (our gold standard), the second is the noisy image, and the third one is the de-noised version of that image. model = load_model_hdf5(&#39;data/RickandMorty/data/models/modelAEND.h5&#39;) cleanvalX &lt;- trainX noisevalX &lt;- trainX fraction_of_pixels &lt;- 0.25 #Generate 100 noisy cases for testing for (i in 1:100) { Npix = prod(dim(noisevalX)[2:3]) Rpix = sample(Npix, fraction_of_pixels * Npix) R &lt;- noisevalX[i,,,1] G &lt;- noisevalX[i,,,2] B &lt;- noisevalX[i,,,3] R[Rpix] &lt;- 0 G[Rpix] &lt;- 0 B[Rpix] &lt;- 0 noisevalX[i,,,1] = R noisevalX[i,,,2] = G noisevalX[i,,,3] = B } predictAEX &lt;- model %&gt;% predict(noisevalX[1:100,,,]) grid::grid.newpage() grid.raster(trainX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(noisevalX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(predictAEX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) "],
["gaussian-process-regression.html", "9 Gaussian process regression", " 9 Gaussian process regression In the previous section we briefly explored fitting multiple polynomials to our data. However, we still had to decide on the order of the polynomial beforehand. A far more powerful approach is Gaussian processes (GP) regression (Williams and Rasmussen 2006). Gaussian process regression represent a Bayesian nonparametric approach to regression capable of inferring nonlinear functions from a set of observations. Within a GP regression setting we assume the following model for the data: \\(y = f(\\mathbf{X})\\) where \\(f(\\cdot)\\) represents an unknown nonlinear function. Formally, Gaussian processes are defined as a collections of random variables, any finite subset of which are jointly Gaussian distributed (Williams and Rasmussen 2006). The significance of this might not be immediately clear, and another way to think of GPs is as an infinite dimensional extension to the standard multivariate normal distribution. In the same way a Gaussian distribution is defined by its mean, \\(\\mathbf{\\mu}\\), and covaraiance matrix, \\(\\mathbf{K}\\), a Gaussian processes is completely defined by its mean function, \\(m(X)\\), and covariance function, \\(k(X,X^\\prime)\\), and we use the notation \\(f(x) \\sim \\mathcal{GP}(m(x), k(x,x^\\prime))\\) to denote that \\(f(X)\\) is drawn from a Gaussian process prior. As it is an infinite dimensional object, dealing directly with the GP prior is not feasible. However, we can make good use of the properties of a Gaussian distributions to sidestep this. Notably, the integral of a Gaussian distribution is itself a Gaussian distribution, which means that if we had a two-dimensional Gaussian distribution (defined over an x-axis and y-axis), we could integrate out the effect of y-axis to give us a (Gaussian) distribution over the x-axis. Gaussian processes share this property, which means that if we are interested only in the distribution of the function at a set of locations, \\(\\mathbf{X}\\) and \\(\\mathbf{X}^*\\), we can specify the distribution of the function over the entirity of the input domain (all of x), and analytically integrate out the effect at all other locations. This induces a natural prior distribution over the output variable that is, itself, Gaussian: \\[ \\begin{eqnarray*} \\begin{pmatrix}\\mathbf{y}^\\top\\\\ \\mathbf{y^*}^\\top \\end{pmatrix} &amp; \\sim &amp; N\\left(\\left[\\begin{array}{c} \\mathbf{0}\\\\ \\mathbf{0}\\\\ \\end{array}\\right],\\left[\\begin{array}{ccc} K(\\mathbf{x},\\mathbf{x}) &amp; K(\\mathbf{x},\\mathbf{x}^*)\\\\ K(\\mathbf{x}^*,\\mathbf{x}) &amp; K(\\mathbf{x}^*,\\mathbf{x}^*) \\\\ \\end{array}\\right)\\right] \\end{eqnarray*} \\] Quite often we deal with noisy data where: \\(y = f(\\mathbf{x}) + \\varepsilon\\), and \\(\\varepsilon\\) represents independent Gaussian noise. In this setting we are interested in inferring the function \\(\\mathbf{f}^*\\) at \\(\\mathbf{X}*\\) i.e., using the noise corrupted data to infer the underlying function, \\(f(\\cdot)\\). To do so we note that a priori we have the following joint distribution: \\[ \\begin{eqnarray*} \\begin{pmatrix}\\mathbf{y}^\\top\\\\ \\mathbf{f^*}^\\top \\end{pmatrix} &amp; \\sim &amp; N\\left(\\left[\\begin{array}{c} \\mathbf{0}\\\\ \\mathbf{0}\\\\ \\end{array}\\right],\\left[\\begin{array}{ccc} K(\\mathbf{x},\\mathbf{x})+\\sigma_n^2 \\mathbb{I} &amp; K(\\mathbf{x},\\mathbf{x}^*)\\\\ K(\\mathbf{x}^*,\\mathbf{x}) &amp; K(\\mathbf{x}^*,\\mathbf{x}^*) \\\\ \\end{array}\\right)\\right] \\end{eqnarray*} \\] 9.0.0.1 Sampling from the prior In the examples below we start by sampling from a GP prior as a way of illustrating what it is that we're actualy doing. We first require a number of packages: require(MASS) require(plyr) require(reshape2) require(ggplot2) Recall that the GP is completely defined by its mean function and covariance function. We can assume a zero-mean function without loss of generality. Until this point, we have not said much about what the covariance function is. In general, the covariance function encodes all information about the type of functions we're interested in: is it smooth? Periodic? Does it have more complex structure? Does it branching? A good starting point, and the most commonly used covariance function, is the squared exponential covariance function: \\(k(X,X^\\prime) = \\sigma^2 \\exp\\biggl{(}\\frac{(X-X^\\prime)^2}{2l^2}\\biggr{)}\\). This encodes for smooth functions (functions that are infinitely differentiable), and has two hyperparameters: a length-scale hyperparameter \\(l\\), which defines how fast the functions change over the input space (in our example this would time), and a process variance hyperparameter, \\(\\sigma\\), which encodes the amplitude of the function (in our examples this represents roughly the amplitude of gene expression levels). In the snippet of code, below, we implement a squared exponential covariance function covSE &lt;- function(X1,X2,l=1,sig=1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) } } return(K) } To get an idea of what this means, we can generate samples from the GP prior at a set of defined positions along \\(X\\). Recall that due to the nature of GPs this is Gaussian distributed: x.star &lt;- seq(-5,5,len=500) ####Define a set of points at which to evaluate the functions sigma &lt;- covSE(x.star,x.star) ###Evaluate the covariance function at those locations, to give the covariance matrix. y1 &lt;- mvrnorm(1, rep(0, length(x.star)), sigma) y2 &lt;- mvrnorm(1, rep(0, length(x.star)), sigma) y3 &lt;- mvrnorm(1, rep(0, length(x.star)), sigma) plot(y1,type = &#39;l&#39;,ylim=c(min(y1,y2,y3),max(y1,y2,y3))) lines(y2) lines(y3) When we specify a GP, we are essentially encoding a distribution over a whole set of functions. Exactly how those functions behave depends upon the choice of covariance function and the hyperparameters. To get a feel for this, try changing the hyperparameters in the above code. What do the functions look like? A variety of other covariance functions exist, and can be found, with examples in the Kernel Cookbook. Exercise 9.4 (optional): Try implementing another covariance function from the Kernel Cookbook and generating samples from the GP prior. Since we have already seen that some of our genes are circadian, a useuful covariance function to try would be the periodic covariance function. 9.0.0.2 Inference with GPs We can generate samples from the GP prior, but what about inference? In linear regression we aimed to infer the parameters, \\(m\\) and \\(a\\). What is the GP doing during inference? Essentially, it's representing the (unknown) function in terms of the observed data and the hyperparameters. Another way to look at it is that we have specified a prior distribution (encoding for all functions of a particular kind) over the input space; during inference in the noise-free case, we then discard all functions that don't pass through those observations. During inference for noisy data we assign greater weight to those functions that pass close to our observed datapoints. Essentially we're using the data to pin down a subset of the prior functions that behave in the appropriate way. For the purpose of inference, we typically have a set of observations, \\(\\mathbf{X}\\), and outputs \\(\\mathbf{y}\\), and are interested in inferring the (unnoisy) values, \\(\\mathbf{f}^*\\), at new set of test locations, \\(\\mathbf{X}^*\\). We can infer a posterior distribution for \\(\\mathbf{f}^*\\) using Bayes' rule: \\(p(\\mathbf{f}^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X}^*) = \\frac{p(\\mathbf{y}, \\mathbf{f}^* | \\mathbf{X}, \\mathbf{X}^*)}{p(\\mathbf{y}|\\mathbf{X})}.\\) A key advantage of GPs is that the preditive distribution is analytically tractible and has the following Gaussian form: \\(\\mathbf{f}^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X}* \\sim \\mathcal{N}(\\hat{f}^*,\\hat{K}^*)\\) where, \\(\\hat{f}^* = K(\\mathbf{X},\\mathbf{X}^*)^\\top(K(\\mathbf{X},\\mathbf{X})+\\sigma^2\\mathbb{I})^{-1} \\mathbf{y}\\), \\(\\hat{K}^* = K(\\mathbf{X}^*,\\mathbf{X}^*)^{-1} - K(\\mathbf{X},\\mathbf{X}^*)^\\top (K(\\mathbf{X},\\mathbf{X})+\\sigma^2\\mathbb{I})^{-1} K(\\mathbf{X},\\mathbf{X}^*)\\). To demonstrate this, let's assume we have an unknown function we want to infer. In our example, for data generation, we will assume this to be \\(y = \\sin(X)\\) as an illustrative example of a nonlinear function (although we know this, the GP will only ever see samples from this function, never the function itself). We might have some observations from this function at a set of input positions \\(X\\) e.g., one observation at \\(x=-2\\): f &lt;- data.frame(x=c(-2), y=sin(c(-2))) We can infer a posterior GP (and plot this against the true underlying function in red): x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y ###Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs ###Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) We can see that the GP has pinned down functions that pass close to the datapoint. Of course, at this stage, the fit is not particularly good, but that's not surprising as we only had one observation. Crucially, we can see that the GP encodes the idea of uncertainty. Although the model fit is not particularly good, we can see exactly where it is no good. Exercise 9.5 (optional): Try plotting some sample function from the posterior GP. Hint: these will be Gaussian distributed with mean {f.star.bar} and covariance {cov.f.star}. Let's start by adding more observations. Here's what the posterior fit looks like if we include 4 observations (at \\(x \\in [-4,-2,0,1]\\)): f &lt;- data.frame(x=c(-4,-2,0,1), y=sin(c(-4,-2,0,1))) x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y ###Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs ###Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) And with \\(7\\) observations: f &lt;- data.frame(x=c(-4,-3,-2,-1,0,1,2), y=sin(c(-4,-3,-2,-1,0,1,2))) x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y ###Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs ###Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) We can see that with \\(7\\) observations the posterior GP has begun to resemble the true (nonlinear) function very well: the mean of the GP lies very close to the true function and, perhaps more importantly, we continue to have an treatment for the uncertainty. 9.0.0.3 Marginal Likelihood and Optimisation of Hyperparameters Another key aspect of GP regression is the ability to analytically evaluate the marginal likelihood, otherwise referred to as the &quot;model evidence&quot;. The marginal likelihood is the probability of generating the observed datasets under the specified prior. For a GP this would be the probability of seeing the observations \\(\\mathbf{X}\\) under a Gaussian distribtion, \\(\\mathcal{N}(\\mathbf{0},K(\\mathbf{X},\\mathbf{X}))\\). The log marginal likelihood for a noise-free model is: \\(\\ln p(\\mathbf{y}|\\mathbf{X}) = -\\frac{1}{2}\\mathbf{y}^\\top [K(\\mathbf{X},\\mathbf{X})+\\sigma_n^2\\mathbb{I}]^{-1} \\mathbf{y} -\\frac{1}{2} \\ln |K(\\mathbf{X},\\mathbf{X})+\\sigma_n^2\\mathbb{I}| - \\frac{n}{2}\\ln 2\\pi\\) We calculate this in the snippet of code, below, hard-coding a small amount of Gaussian noise: calcML &lt;- function(f,l=1,sig=1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+0.1^2*diag(length(y)))%*%y -0.5*log(det(K)) -(length(f[,1])/2)*log(2*pi); return(ML) } The ability to calculate the marginal likelihood gives us a way to automatically select the hyperparameters. We can increment hyperparameters over a range of values, and choose the values that yield the greatest marginal likelihood. In the example, below, we increment both the length-scale and process variance hyperparameter: library(plot3D) par &lt;- seq(.1,10,by=0.1) ML &lt;- matrix(rep(0, length(par)^2), nrow=length(par), ncol=length(par)) for(i in 1:length(par)) { for(j in 1:length(par)) { ML[i,j] &lt;- calcML(f,par[i],par[j]) } } persp3D(z = ML,theta = 120) ind&lt;-which(ML==max(ML), arr.ind=TRUE) print(c(&quot;length-scale&quot;, par[ind[1]])) print(c(&quot;process variance&quot;, par[ind[2]])) Here we have performed a grid search to identify the optimal hyperparameters. In practice, the derivative of the marginal likelihood with respect to the hyperparameters is analytically tractable, allowing us to optimise using gradient search algorithms. Exercise 9.7: Now try fitting a Gaussian process to one of the gene expression profiles in the Botrytis dataset. Hint: You may need to normalise the time axis. Since this data also contains a high level of noise you will also need to use a covariance function/ML calculation that incorporates noise. The snippet of code, below, does this, with the noise now representing a \\(3\\)rd hyperparameter. covSEn &lt;- function(X1,X2,l=1,sig=1,sigman=0.1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) if (i==j){ K[i,j] &lt;- K[i,j] + sigman^2 } } } return(K) } calcMLn &lt;- function(f,l=1,sig=1,sigman=0.1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+diag(length(y))*sigman^2)%*%y -0.5*log(det(K+diag(length(y))*sigman^2)) -(length(f[,1])/2)*log(2*pi); return(ML) } 9.0.0.4 Model Selection As well as being a useful criterion for selecting hyperparameters, the marginal likelihood can be used as a basis for selecting models. For example, we might be interested in comparing how well we fit the data using two different covariance functions: a squared exponential covariance function (model 1, \\(M_1\\)) versus a periodic covariance function (model 2, \\(M_2\\)). By taking the ratio of the marginal likelihoods we can calculate the Bayes' Factor (BF) which allows us to determine which model is the best: \\(\\mbox{BF} = \\frac{ML(M_1)}{ML(M_2)}\\). High values for the BF indicate strong evidence for \\(M_1\\) over \\(M_2\\), whilst low values would indicate the contrary. Excercise 3.1: Using our previous example, \\(y = sin(x)\\) try fitting a periodic covariance function. How well does it generalise e.g., how well does it fit \\(f(\\cdot)\\) far from the observation data? How does this compare to a squared-exponential? Example covariance functions implemented from the Kernel Cookbook. Here we implement a rational quadratic covariance function: covRQ &lt;- function(X1,X2,l=1,sig=1,a=2) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*(1 + (abs(X1[i]-X2[j])^2/(2*a*l^2)) )^a } } return(K) } Here we implement a periodic covariance function: covPer &lt;- function(X1,X2,l=1,sig=1,p=1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(sin(pi*abs(X1[i]-X2[j])/p)^2 / l^2) } } return(K) } We need to borrow the following snippets of code from the main text. require(MASS) require(plyr) require(reshape2) require(ggplot2) covSE &lt;- function(X1,X2,l=1,sig=1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) } } return(K) } x.star &lt;- seq(-5,5,len=500) f &lt;- data.frame(x=c(-4,-3,-2,-1,0,1,2), y=sin(c(-4,-3,-2,-1,0,1,2))) x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y #Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var y1 &lt;- mvrnorm(1, f.star.bar, cov.f.star) y2 &lt;- mvrnorm(1, f.star.bar, cov.f.star) y3 &lt;- mvrnorm(1, f.star.bar, cov.f.star) plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type = &#39;p&#39;,col=&quot;blue&quot;) lines(x.star,y1,type = &#39;l&#39;,col=&quot;blue&quot;) lines(x.star,y2,type = &#39;l&#39;,col=&quot;blue&quot;) lines(x.star,y3,type = &#39;l&#39;,col=&quot;blue&quot;) calcML &lt;- function(f,l=1,sig=1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+0.1^2*diag(length(y)))%*%y -0.5*log(det(K)) -(length(f[,1])/2)*log(2*pi); return(ML) } #install.packages(&quot;plot3D&quot;) library(plot3D) par &lt;- seq(.1,10,by=0.1) ML &lt;- matrix(rep(0, length(par)^2), nrow=length(par), ncol=length(par)) for(i in 1:length(par)) { for(j in 1:length(par)) { ML[i,j] &lt;- calcML(f,par[i],par[j]) } } ind&lt;-which(ML==max(ML), arr.ind=TRUE) lmap&lt;-par[ind[1]] varmap&lt;-par[ind[2]] x.star &lt;- seq(-5,5,len=500) f &lt;- data.frame(x=c(-4,-3,-2,-1,0,1,2), y=sin(c(-4,-3,-2,-1,0,1,2))) x &lt;- f$x k.xx &lt;- covSE(x,x,lmap,varmap) k.xxs &lt;- covSE(x,x.star,lmap,varmap) k.xsx &lt;- covSE(x.star,x,lmap,varmap) k.xsxs &lt;- covSE(x.star,x.star,lmap,varmap) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y #Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) Now try fitting a Gaussian process to one of the gene expression profiles in the Botrytis dataset. covSEn &lt;- function(X1,X2,l=1,sig=1,sigman=0.1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) if (i==j){ K[i,j] &lt;- K[i,j] + sigman^2 } } } return(K) } geneindex &lt;- 36 lmap &lt;- 0.1 varmap &lt;- 5 x.star &lt;- seq(0,1,len=500) f &lt;- data.frame(x=D[25:nrow(D),1]/48, y=D[25:nrow(D),geneindex]) x &lt;- f$x k.xx &lt;- covSEn(x,x,lmap,varmap,0.2) k.xxs &lt;- covSEn(x,x.star,lmap,varmap,0.2) k.xsx &lt;- covSEn(x.star,x,lmap,varmap,0.2) k.xsxs &lt;- covSEn(x.star,x.star,lmap,varmap,0.2) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y #Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var plot(f,type = &#39;l&#39;,col=&quot;red&quot;) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) calcMLn &lt;- function(f,l=1,sig=1,sigman=0.1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+diag(length(y))*sigman^2)%*%y -0.5*log(det(K+diag(length(y))*sigman^2)) -(length(f[,1])/2)*log(2*pi); return(ML) } 9.0.0.5 Scalability Whilst GPs represent a powerful approach to nonlinear regression, they do have some limitations. GPs do not scale well with the number of observations, and standard GP approaches are not suitable when we have a very large datasets (thousands of observations). To overcome these limitations, approximate approaches to inference with GPs have been developed. Exercise 3.3: Write a function for determining differential expression for two genes. Hint: we are interested in comparing two models, and using Bayes' Factor to determine if the genes are differentially expressed. #### Advanced application 1: differential expression of time series {#application-1} Differential expression analysis is concerned with identifying if two sets of data are significantly different from one another. For example, if we measured the expression level of a gene in two different conditions (control versus treatment), you could use an appropriate statistical test to determine whether the expression of that gene had been affected by the treatment. Most statistical tests used for this are not appropriate when dealing with time series data (illustrated in Figure 9.1). Figure 9.1: Differential expression analysis for time series. Here we have two time series with very different behaviour (right). However, as a whole the mean and variance of the time series is identical (left) and the datasets are not differentially expressed using a t-test (p&lt;0.9901) Gaussian processes regression represents a useful way of modelling time series, and can therefore be used as a basis for detecting differential expression in time series. To do so we write down two competing modes: (i) the two time series are differentially expressed, and are therefore best described by two independent GPs; (ii) the two time series are noisy observations from an identical underlying process, and are therefore best described by a single joint GP applied to the union of the data. Exercise 3.2 (optional): Write a function for determining differential expression for two genes. Hint: you will need to fit \\(3\\) GPs: one to the mock/control, one to the infected dataset, and one to the union of mock/control and infected. You can use the Bayes' Factor to determine if the gene is differentially expressed. f &lt;- data.frame(x=D[25:nrow(D),1]/48, y=D[25:nrow(D),geneindex]) par &lt;- seq(.1,10,by=0.1) ML &lt;- matrix(rep(0, length(par)^2), nrow=length(par), ncol=length(par)) for(i in 1:length(par)) { for(j in 1:length(par)) { ML[i,j] &lt;- calcMLn(f,par[i],par[j],0.05) } } persp3D(z = ML,theta = 120) ind&lt;-which(ML==max(ML), arr.ind=TRUE) Now let's calculate the BF. lmap &lt;- par[ind[1]] varmap &lt;- par[ind[2]] f1 &lt;- data.frame(x=D[1:24,1]/48, y=D[1:24,geneindex]) f2 &lt;- data.frame(x=D[25:nrow(D),1]/48, y=D[25:nrow(D),geneindex]) f3 &lt;- data.frame(x=D[,1]/48, y=D[,geneindex]) MLs &lt;- matrix(rep(0, 3, nrow=3)) MLs[1] &lt;- calcMLn(f1,lmap,varmap,0.05) MLs[2] &lt;- calcMLn(f2,lmap,varmap,0.05) MLs[3] &lt;- calcMLn(f3,lmap,varmap,0.05) BF &lt;- (MLs[1]+MLs[2]) -MLs[3] BF So from the Bayes' Factor there's some slight evidence for model 1 (differential expression) over model 2 (non-differential expression). "]
]
